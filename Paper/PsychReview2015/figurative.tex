%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa6.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%% 
%% apa6 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 6th edition
%% 
%% Copyright (C) 2011-2014 by Brian D. Beitzel <brian at beitzel.com>
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%% 
%% http://www.latex-project.org/lppl.txt
%% 
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%% 
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%% 
%% ----------------------------------------------------------------------
%% 
\documentclass[jou]{apa6}
\usepackage{stmaryrd}
\usepackage[table]{xcolor}
\usepackage{apacite}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{url}
%\usepackage{gb4e}

\usepackage{amsmath,epsfig}

%\usepackage[american]{babel}

\usepackage{csquotes}
%\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\DeclareLanguageMapping{american}{american-apa}
%\addbibresource{bibliography.bib}

\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}
\newcommand{\jtk}[1]{\textcolor{Blue}{[jtk: #1]}}


\title{Modeling the Pragmatics of Figurative Language Understanding\ndg{better title?}}
\shorttitle{Pragmatics of Figurative Language}

\author{Justine T. Kao, Leon Bergen, Noah D. Goodman}
\affiliation{Stanford University}


\leftheader{Kao, Bergen, Goodman}

\abstract{People use language to convey information that goes far beyond an utterance's literal meaning. In particular, figurative language such as hyperbole, irony, and metaphor showcases people's ability to 
%incorporate background knowledge and social reasoning to 
infer relevant and true information from utterances that are false under their literal semantics. 
% and context to derive linguistic meaning.
%For example, metaphors highlight specific similarities between disparate concepts; hyperbolic statements communicate speakers’ attitudes and opinions without explicitly stating them. 
%While figurative utterances are often literally false, people are highly adept at inferring relevant and true information from these utterances. 
%What is the cognitive and linguistic basis of figurative language understanding?
%People routinely go beyond the literal meanings of utterances to infer speakers' intended meanings in context. In particular, figurative language such as hyperbole, irony, and metaphor showcase people's ability to use background knowledge and context to derive linguistic meaning.
Although figurative language is prevalent in communication and has been studied across many fields, how people arrive at appropriate interpretations of figurative utterances remains unclear.
%using contextual and background information
Here we describe a computational model that formalizes figurative communication as recursive social reasoning between speaker and listener. Our model extends and overcomes limitations of basic Rational Speech Act (RSA) models, a family of computational models that capture many phenomena in human pragmatic reasoning but are restricted to the interpretation of literally true utterances.
%Our model incorporates uncertainty about the speaker's communicative goal, otherwise known as the question under discussion (QUD). 
We show that an RSA model extended to accommodate inferences about the question under discussion (QUD) is able to predict people's interpretations of hyperbole, irony, and metaphor. We argue that despite apparent differences among subtypes of figurative language, the same pragmatic framework flexibly produces fine-grained interpretations for a range of figurative uses. We use this as evidence suggesting that the rich and often affectively-laden meanings expressed by figurative language can be explained by basic principles of communication.
%Understanding others' thoughts through the words they speak requires integrating a host of information sources to create meaning. figurative language  --- sentences whose intended meanings differ significantly from their literal meanings --- raises questions about the linguistic, cognitive, and social faculties that enable people to go beyond linguistic input and reason about others' intentions in understand utterances in context. 
}

\keywords{pragmatics, figurative language, computational modeling}

\authornote{Justine T. Kao, Department of Psychology,
  Stanford University.
  
  Leon Bergen, Department of Psychology, Stanford University.
  
  Noah D. Goodman, Department of Psychology, Stanford University.

  Correspondence concerning this article should be addressed to Justine T. Kao, Department of Psychology, Stanford University, 450 Serra Mall, Stanford, CA 94025.  E-mail: justinek@stanford.edu}

\begin{document}
\maketitle
%\ndg{why is figurative language important? very common, perhaps central to creativity, a key challenge to `logic-like' views of mind, ...}
%Imagine a world where people always say exactly what they mean. What would that be like? There would be fewer misunderstandings, perhaps, but communication would feel stunted and dull, losing much of its current flavor.
%In the world we live in, 
People do not always mean what they say---we implicate, exaggerate, make metaphors, and even wax poetic. 
One of the most challenging puzzles in communication is how people go beyond the words a speaker produces to infer the underlying meaning a speaker intends. 
Although intended meanings differ from overt meanings in many instances of language use, the effect is most striking in the case of figurative language. 
%The ability to understand figurative language is necessary in a world where people do not always mean what they say. 
From ``Juliet is the sun'' to ``It took a million years to write this paper,'' figurative language such as hyperbole, sarcasm, and metaphor evokes sensations of poetic beauty or humor by intentionally producing meanings that differ dramatically from (and often explicitly contradict) their literal meanings \cite{glucksberg2001understanding, pilkington2000poetic, lakoff2009more, roberts1994people}. 
%People often use language to convey information that goes far beyond a sentence's literal meaning. Hyperbolic statements communicate speakers' attitudes and opinions by exaggerating the truth (``It took a million years to write this paper''); ironic or sarcastic statements communicate meanings contrary to what is encoded in the literal semantics (``Paper-writing is my favorite activity''); and metaphors highlight hidden similarities between distinct categories by equating them (``Paper-writing is a marathon''). 
This radical departure from the standard encoded meanings of utterances is partly why figurative language has intrigued philosophers, linguists, psychologists, and literary theorists for decades, sparking debates about how we understand figurative language, why we appreciate it, and what it reveals about the human mind \cite{glucksberg2001understanding, papafragou1996figurative, li2010using, kreuz1993empirical}.

Although early accounts of figurative language treat it as a literary flourish that primarily serves an ornamental role \cite{gibbs1994poetics}, more recently the dominant view is that figurative language is highly prevalent in everyday use and reflects much of how we perceive and experience the world. This shift traces back to \citeA{lakoff2008metaphors}'s influential work on conceptual metaphor, which identified conceptual metaphors that form the basis of many of our common expressions (e.g. describing time as ``long'' makes use of the conceptual mapping between \textsc{time} and \textsc{space}). A great deal of work that followed continued to surface the figurative underpinnings of everyday language and cognition \cite{gibbs1994poetics, lakoff2009more, katz1998figurative}. At the same time, despite the now broadly accepted idea that figurative language can occur in many (sometimes mundane) ways in our daily use, figurative language is still intimately connected to creativity through its ability to reveal hidden similarities between concepts and to add flavor and interest to communication. \jtk{maybe more on creativity...}

Given its prevalence and importance, figurative language poses a key challenge to theories of communication in which speakers are construed as rational and cooperative agents. In the field of pragmatics, communication is viewed as being governed by a set of maxims and rational principles, which together explain many diverse phenomena in language understanding \cite{grice20134, levinson1983pragmatics}. However, figurative language use does not appear to adhere to these rational principles in a straightforward manner. 
%principles appear to break down when analyzing figurative language. 
If a rational speaker's goal is to transfer information as efficiently as possible to a listener, why would she choose to produce an utterance whose literal meaning directly contradicts her intended meaning? Furthermore, how could a listener receive an overtly false utterance and recover the speaker's intended meaning, without abandoning the assumption that the speaker behaves in a cooperative and informative manner?

%Given the puzzling nature of figurative language and the tension it represents between creativity in thought and rationality in communication
While researchers agree that figurative language is puzzling, there has been disagreement about how precisely we should approach the problem of figurative meaning \cite{gibbs2012interpreting, gibbs2001evaluating}. Some researchers 
%discard the notion that there is a single unified phenomena of figurative language, and instead 
focus on uncovering mechanisms that underly specific tropes such as metaphor and irony \cite{tourangeau1982understanding, colston2002irony}. Other researchers maintain that figurative language can be explained by more general theories of communication, and that figurative language lies on a spectrum of other common uses that deviate from literal meanings to varying degrees \cite{searle1979metaphor, sperber2008deflationary}. In this paper, we take the latter view and argue that interpretations of figurative language are shaped by basic principles of communication. Furthermore, we show that figurative meaning is made possible precisely because these principles allow listeners to combine information sources in a systematic manner and make inferences that go beyond the overt literal meanings of words. We also argue that a precise, quantitative (and hence computational) approach is desirable in order to formalize these principles and evaluate them against empirical data. 

Although computational models exist for specific subtypes of figurative language, especially metaphor \cite{falkenhainer1989structure, kintsch2000metaphor}, to our knowledge there has not yet been a computational framework that formalizes pragmatic theories and explains figurative language in the general case. Constraint-based approaches (e.g. \citeA{katz2001moment}) aim to provide a general theory of figurative language by specifying various constraints that influence interpretation of a figurative utterance in context. However, these approaches do not describe precisely how the factors interact or posit a principled way of explaining those interactions. We believe that a computational, theory-driven framework could clarify how various factors interact to shape figurative meaning and situate figurative language in a more general theory of language understanding.

Our goal in this paper is to propose an explicit and testable theory of figurative language and empirically evaluate it using a set of diverse examples. We describe a computational model that formalizes a central principle in the Gricean view of communication: Listeners reason about speakers and assume them to be rational agents who choose utterances that are maximally informative and efficient \cite{grice20134}. Critically, we also make explicit the idea that listeners expect speakers' utterances to address a (possibly implicit) topic of conversation, thus formalizing a notion of relevance \cite{sperber1986relevance}. 
%Because listeners can be uncertain about the exact topic of conversation, they also reason about the topic of conversation. 
By jointly inferring the topic and interpretation given an utterance, a listener can arrive at meanings that are relevant to a topic but that deviate from the utterance's literal meaning, thus producing figurative interpretations through the standard course of language understanding. 
%Although figurative statements are often false under their literal semantics (Juliet is not literally the sun, and it is infeasible to take a million years to write a paper), people are highly adept at inferring relevant and true information from these utterances. 
%%Because the literal meanings of these utterances are insufficient for uncovering the intended meanings, understanding figurative language requires integrating a host of information sources to create meaning. 
%This ability to go beyond direct evidence (the words) to infer unobserved information (the meanings) is a hallmark of human intelligence that underlies many aspects of how we understand and interact with the world. How do our linguistic, cognitive, and social faculties work together to allow us to fluently and accurately understand the communicative intent behind figurative utterances?

%\ndg{this paragraph seems to say the same thing a few times, but also isn't very clear about what a literal or figurative meaning actually are. we don't actually want to define them yet, but we also don't want people to bump on this... maybe start with the observation that intended, and received, meanings often differ from overt meaning; this seems to be most radically true in the case of figurative language.}

%Figurative language is important because...
%\ndg{we should argue that a precise, quantitative (and hence computational) approach is to be desired. these exist (somewhat) for the specific cases (eg SME) but not for the pragmatic theories.}

%Taking the approach of analyzing general communicative principles that shape interpretations of figurative language, our goal in this paper is to propose an explicit and testable theory of figurative language understanding that can be validated against empirical data. We describe a computational model that integrates several pragmatic elements (e.g. assumptions that speakers are rational and cooperative; assumptions that utterances tend to be informative and relevant to the topic of conversation; representations of common knowledge and prior beliefs; and inferences about speakers' subjective attitudes) to produce appropriate interpretations of figurative utterances. 

%\ndg{give a bit more intuition about how our model will work. it formalizes the gricean view, but makes the topic explicit and central (a la relevance). the key insight is that by jointly inferring the topic and interpretation, a listener can deviate from literal meaning through the standard course of utterance interpretation.}

In what follows, we first review core empirical phenomena in figurative language, highlighting existing research and open questions regarding factors that shape figurative language understanding. Next, we describe the ways in which many of these factors can be integrated through a framework of pragmatic reasoning. We introduce Rational Speech-Acts (RSA) models, a family of computational models that formalizes communication as recursive social reasoning \cite{frank2012predicting, goodman2013knowledge}. We then show that natural but critical extensions can be made to RSA models to account for inferences about the topic of conversation via a principle of relevance. We evaluate the extended model on three types of figurative language: hyperbole, verbal irony, and metaphor, and present behavioral data and modeling results. Finally, we discuss the insights this model reveals about figurative language understanding as well as future research that our modeling approach licenses. We argue that a general model of figurative language enables us to more precisely examine the ways in which semantics and principles of communication interact to generate rich linguistic meaning.


%An ocean of ink has been spilled attempting to answer this question, across many disciplines including psychology, linguistics, philosophy, computer science, and literary theory
%\section{Figurative Language: The Phenomena}
\section{Previous Approaches to Figurative Language}
\jtk{better headings}
%In this section, we will review previous approaches to studying figurative language in psycholinguistics and pragmatics. 
\subsection{Trope-Specific Mechanisms}
Much of the  empirical research on figurative language focuses on cognitive mechanisms that underly interpretations of specific types of figurative use. In order to explain how metaphors are processed and understood, psychologists proposed various ways in which people align shared properties and analogous relations across different domains, including the domain interaction model \cite{tourangeau1982understanding}, structure mapping model \cite{gentner1997alignment, gentner1983structure}, and category assertion model \cite{glucksberg2003psycholinguistics}. These models offer different predictions of metaphor interpretation in various contexts and are often distinguished through carefully designed behavioral experiments. In order to explain other types of figurative language, separate accounts are then posited, such as the pretense theory \cite{clark1984pretense} and the echoic mention theory \cite{sperber1981irony} for verbal irony.
While these trope-specific approaches significantly advance our understanding of the cognitive factors invoked by particular types of figurative language, they require an array of distinct mechanisms in addition to those involved in standard language understanding. 
Furthermore, these approaches often leave open the question of how particular figurative utterances trigger these specialized mechanisms in the first place.
%, or why, if figurative utterances require an additional processing step, it is often just as easy to understand these utterances as their literal paraphrases in certain contexts.
\subsection{General Theories of Figurative Language}
A different approach to studying figurative language focuses on how people use general communicative principles to arrive at contextually appropriate interpretations. This type of approach is more commonly found in the pragmatics literature, which we review below. 
%\cite{grice20134, searle1979metaphor, sperber2008deflationary, ortony1993metaphor, tendahl2008complementary}. We briefly review two main bodies of work that have taken this approach to explain figurative language.
\subsubsection{Standard Pragmatic View}
A classic theory of figurative interpretation in the pragmatics literature is the standard pragmatic view, which analyzes figurative utterances using standard Gricean maxims. \citeA{grice20134} proposed that speakers produce utterances that adhere to principles of quality (truthfulness), quantity (informativeness), relevance, and manner (e.g. brevity, orderliness, clarity), and that listeners interpret utterances with the assumption that speakers are cooperative and follow these maxims. Under this view, figurative utterances are understood through a three-step process: (1) determine the literal meaning of the utterance (2) determine whether the literal meaning violates the quality maxim by being untruthful (3) reanalyze the utterance to identify implied or figurative meanings that would allow the utterance to adhere to the Gricean maxims \cite{grice20134, searle1979metaphor}.

Although the standard pragmatic view is appealing in that it fits parsimoniously within a more general theory of language understanding, it has met with several criticisms. One of the critiques is the fact that many figurative statements do not violate the quality maxim because their literal meanings are true to begin with. ``No man is an island,'' for example, is a literally true statement in addition to a metaphorically meaningful one  \cite{gibbs1992metaphor}. By relying on the violation of the quality maxim, the standard pragmatic view does not provide a satisfying explanation for how figurative meanings arise from these types of utterances. 
An even more common criticism of the standard pragmatic view is that it requires the listener to first access the literal meaning of the utterance, verify that the literal meaning is false, compute potential figurative interpretations, and then select the interpretation that best satisfies conversational maxims. Given the extra steps involved, this theory predicts that people should take longer to interpret figurative utterances than literal utterances. However, many experiments in the psycholinguistics literature have shown that the figurative meanings of irony and metaphor can be accessed as quickly or even more quickly than their literal meanings given supporting contexts \cite{glucksberg2003psycholinguistics, gildea1983understanding, gibbs1992metaphor}. These empirical findings suggest that literal meanings do not have to be explicitly computed and rejected before appropriate figurative interpretations emerge, leading to decreased popularity of the standard pragmatic view.

\subsubsection{Relevance Theory}
%The central claim of relevance theory is that human cognition is governed by a tendency to maximize relevance 
%\citeA{sperber2008deflationary} introduced the communicative principle of relevance, which is that ``every act of inferential communication conveys a presumption of its own optimal relevance.''
Instead of using Grice's four maxims as the guiding principle for figurative interpretation, relevance theory proposes that the principle of relevance is sufficient for explaining a range of phenomena in communication and cognition more generally. 
Relevance theorists claim that the interpretation of all language involves maximizing the relevance of the interpretation to a contextually determined topic \cite{sperber2008deflationary, tendahl2008complementary, sperber1986relevance}. 
%As a result, interpretations of the same utterance can vary dramatically given different topics.
%Suppose Ann and Bob are discussing their friend Cam. Ann asks, ``Does Cam have a fever?'' and Bob replies, ``He is boiling.'' Ann will interpret Bob's utterance to mean that Cam has a very high temperature. If, on the other hand, Ann asks, ``Is Cam upset?'' and Bob replies, ``He's boiling,'' Ann should interpret Bob's utterance to mean that Cam is very angry. The word ``boiling'' receives different figurative meanings in these two contexts. Ann arrives at the appropriate interpretation by assuming that Bob's utterance provides maximally relevant information regarding her question. 
Under this view, figurative uses such as hyperbole and metaphor are not distinct from literal language, but rather lie on a continuum of ``loose uses'' that require listeners to use the principle of relevance to recover the intended meanings. 
This view situates figurative language within a general  theory of communication and has been argued to provide a complementary perspective to cognitive linguistics in the study of metaphor \cite{tendahl2008complementary}. However, one concern with relevance theory is that the concept of relevance, while intuitively appealing, has not been clearly operationalized or tested in a quantitative manner to determine its specific role in language understanding. 

In this paper, we formalize general communicative principles principles and show that they capture fine-grained interpretations of specific figurative tropes. The first principle is that speakers and listeners communicate with the assumption that their interlocutors are rational and cooperative agents, where speakers aim to be maximally informative and efficient in their choice of utterance \cite{grice20134}. The second principle is a particular notion of the relevance principle \cite{sperber1986relevance}, which is that speakers choose utterances to maximize informativeness with respect to specific communicative goals. Third, speakers and listeners utilize their shared knowledge of the world to produce and interpret utterances. In addition to formalizing general pragmatic principles, we also make connections with trope-specific findings by identifying communicative goals that are best addressed by different types of figurative uses.

%Here we present a computational model that formalizes and integrates three general principles of language and communication to explain the basis of nonliteral language understanding. First, speakers and listeners communicate with the assumption that their interlocutors are rational and cooperative agents; second, listeners assume that speakers choose utterances to maximize informativeness with respect to their communicative goals; third, speaker and listener utilize common ground---their shared knowledge of the world---to communicate effectively. 


%\ndg{maybe some of the details of standard and relevance theories (and arguments about them) should be saved for the next section?}

%\section{A proposal}

%In the next chapter, I will describe such a formal model and argue that it may illuminate our understanding figurative communication.
%
%
%ARGUE FOR NOT ENOUGH EXPLICITNESS.
%While both the standard pragmatic and relevance theoretic view provide useful frameworks for understanding the pragmatics of figurative communication, certain factors that are important in general communication remain underspecified. For example, while relevance theory considers which interpretations are maximally relevant to the question under discussion, it does not provide a clear operationalization of relevance. 
%
%neither the standard pragmatic nor relevance theoretic view explicitly take into account the speaker's desire to be informative. There is also little consideration of the shared encyclopedic knowledge associated with different utterances, or representation of the listener's prior beliefs. WWe believe that an adequate model of figurative language understanding should flexibly integrate these components to determine an appropriate interpretation. In addition, figurative language is often used to  express subjective experiences and emotional attitudes \cite{riloff2005exploiting, roberts1994people}.
%Since these emotional subtexts contribute greatly to the appeal of figurative language, it is important for a theory of figurative language understanding to include analyses of these effects.
%
%Here we adopt an approach closer to the latter, with the goal of proposing a general pragmatic framework that explains the basis of figurative communication.


\section{Figurative Language: The Phenomena} 
%\todo{include list of types of figurative language?}


%Under this definition, an utterance such as ``That chocolate cake is to die for'' is figurative, because the speaker does not mean that the chocolate cake is literally worth giving up one's life. Perhaps less obviously, an indirect speech act such as ``Can you pass the salt?'' is also figurative, because the intended meaning of the utterance differs from the literal meaning of the question---the speaker does not mean to inquire about the listener's ability to pass the salt.
%At first glance, this definition seems straightforward and corresponds with our intuitions regarding which usages of language are literal and which are figurative; however, it grows murky upon closer inspection. Suppose a speaker Bob says, ``I arrived late and the theater was full.'' Since it is implausible that the entire space of the theater was occupied from floor to ceiling, the sentence's strict literal meaning appears to be false. Instead, Bob most likely intends to communicate that he had difficulty finding an empty seat at the theater. This is an example of ``loose talk,'' also described as pragmatic slack, where a speaker uses a proposition $Q$ (e.g. the theater was full) in order to communicate a set of propositions that can be derived from $Q$ (e.g. there were a lot of people at the theater, and Bob was unable to find a seat), without being committed to the truthfulness of $Q$ \cite{sperber1985loose, lasersohn1999pragmatic, bach1994semantic}. 
%%Based on the naive definition of figurative language, the utterance ``The theater was full'' can be characterized as not strictly literal and thus figurative.
%%A speaker wishes to communicate a set of propositions $P_1 \dots P_n$ (e.g. there were a lot of people at the auditorium; we weren't able to find seats), which are derivable from a stricter proposition $Q$ (e.g. the auditorium was full). In order to minimize effort while effectively communicating $P_1 \dots P_n$, the speaker chooses to utter $Q$ without fully committing to its truth. Relevance theorists argue that the relevance principle---the assumption that speakers choose utterances to maximize relevance---is what allows listeners to correctly infer $P_1 \dots P_n$ without necessarily believing that $Q$ \cite{sperber1985loose}.
% 
%On the other hand, consider an utterance such as, ``Bob is always late,'' produced by an annoyed speaker. In order for the literal meaning of the utterance to be true, for all cases in which Bob can be either late or on time, Bob must be late in $100\%$ of the cases. However, one can easily interpret the utterance to mean that the speaker thinks Bob is very often (but not literally always) late, thus arriving at an interpretation that differs from the literal meaning of the utterance.
%Under these analyses, the intended meanings of both of these utterances (``The theater was full'' and ``Bob is always late'') differ from their ``literal'' meanings. Indeed, \citeA{sperber1985loose} claim that there is no discontinuity between loose and figurative uses of language; both exploit the principle of relevance in order to express what is derivable from the utterance without committing to the truth of the literal meaning of the utterance. At the same time, a sentence such as ``Bob is always late'' feels qualitatively different from a sentence such as ``The theater was full'' and is more easily recognized as hyperbole. In fact, it has been observed that some utterances are intuitively recognized as ``figurative'' while other are not \cite{coulson2005blending}, which suggests that figurative language may be a psychologically meaningful category distinct from most other loose uses.


%The definition of figurative language is further complicated when taking polysemy into account \cite{gibbs2012interpreting}. Consider the different senses of the word ``full.'' Suppose someone says, ``I am full'' after a meal. Since the speaker does not mean that the space inside her body is literally filled up, the sentence is not true under one particular literal meaning of ``full.'' However, one might argue that ``full'' has several different senses, and that the sense of being satisfied with food is the sense that is used literally in this utterance. One could then further modify the literal semantics of ``full'' for an utterance such as ``My heart is full (of emotion),'' such that the literal meaning of ``full'' includes a physical sensation in one's chest that roughly corresponds to experiencing emotion. In other words, one could keep adding different senses to the literal meaning of words until no usage is ever figurative. 
%Once one becomes sufficiently careful, it appears that the boundary between literal and figurative language is extremely fuzzy. At the same time, the fact that a great deal of research has targeted figurative language as a distinct category of language use, and that some utterances are intuitively recognized as ``figurative'' while other are not \cite{coulson2005blending}, suggests that such figurative language is a psychologically meaningful category. 
%%Where does this intuitive sense of ``figurative-ness'' come from? Do sentences have well-defined literal meanings, and is the distinction between literal and figurative meanings psychologically meaningful?  
%Instead of seeking to define the precise boundary between literal and figurative meanings, here we propose a weaker and less controversial definition. We suggest that the ``figurativeness'' of an utterance depends upon how Perhaps this intuition is based not on a clearcut boundary between literal and figurative meanings, but rather on a graded \emph{distance} between interpretations that the same utterance generates. Consider an utterance such as ``His car cost ten thousand dollars.'' Given most contexts that one can easily bring to mind, a reasonable interpretation of this utterance is that the car cost exactly $\$10,000$, or the car cost approximately $\$10,000$, and it is difficult to come up with a third interpretation. These two interpretations are not very different from each other. However, consider an utterance such as ``His cup of coffee cost ten thousand dollars.'' This utterance could also be interpreted as the coffee cost exactly $\$10,000$ or approximately $\$10,000$. However, one can easily take into account background knowledge (a cup of coffee almost never costs $\sim \$10,000$) and speaker intention (the speaker may want to express an attitude about the coffee and not its exact price) to conclude that the coffee cost $ << \$10,000$ and that the speaker believes the coffee is too expensive. Regardless of which interpretation is strictly literal, it is clear that the third interpretation is quite different from the other two. The fact that the utterance about the coffee generates distinct interpretations and the utterance about the used car generates similar interpretations may give rise to the intuitive sense that distinguishes figurative from normal utterances. In what follows, we will label an utterance as ``figurative'' if it intuitively generates distinct interpretations and leave it to future work to quantify this sense of ``distinctiveness.'' 

%Despite many efforts to draw a distinction between literal and figurative language, the line remains blurry \cite{honeck1986verbal, coulson2005blending}. In fact, many researchers have argued that the line does not exist, partly due to the fact that literal meaning itself is not a single cohesive notion \cite{gibbs1994poetics, lakoff1986meanings, giora2002literal, ariel2002demise}. Instead of seeking to define the precise boundary between literal and figurative meanings, here we will focus on cases that are rather uncontroversially categorized as ``figurative.'' In order to identify these cases, we first review the various types of language use that researchers have included within the category of figurative language and extract overlapping cases.

%%
Despite general consensus that figurative language consists of utterances that differ in various ways from their ``literal'' or standard meanings \cite{gibbs1999figurative}, researchers have not always agreed on which tropes should be included in the category of figurative language \cite{honeck1986verbal, kreuz1993empirical}. Here we identify different types of figurative language, describe related empirical phenomena, and review factors that have been shown to affect figurative interpretation.

\subsection{Types of figurative language}
%In part due to the difficulty of defining figurative language, 

%\citeA{lanham1991handlist} created a list of nearly $1000$ rhetorical terms, while 
\citeA{kreuz1993empirical} identified eight figures that they believed form the basic categories of figurative language: \emph{indirect requests}, which are commands phrased as comments or questions; 
%(e.g. ``It would be great if you could keep this a secret'')
\emph{idioms}, where the intended meaning of the utterance cannot be derived from the individual words' typical meanings; %(e.g. ``Ann ended up spilling the beans'')
\emph{irony}, where the intended meaning is opposite in polarity from the utterance's literal meaning; 
%(e.g. ``Ann is the best secret keeper ever'', in a situation where Ann clearly failed to keep a secret)
\emph{understatement}, where the speaker intentionally says something that is less extreme or intense than is actually the case;  %(e.g. ``Bob seems a tiny bit upset at Ann'', when Bob is clearly furious)
\emph{hyperbole}, where a speaker intentionally says something that is more extreme or intense than is actually the case; 
%(e.g. ``Bob won't forgive Ann in a million years'')
\emph{metaphor}, where concepts from distinct domains are implicitly compared or equated with each other; 
%(e.g. ``Bob's anger is a tornado'')
\emph{simile}, where concepts from distinct domains are explicitly compared; 
%(e.g. ``Bob's anger is like a fire'')
and \emph{rhetorical questions}, which are questions that do not require an answer.
%(e.g .``What was Ann thinking giving away that secret?'')
 \citeA{gibbs1999figurative} agreed with most of the figures while excluding \emph{rhetorical questions} and including \emph{metonymy}, \emph{proverbs}, and \emph{oxymora}. Based on these lists and on the amount of attention each figurative trope has received in the psycholinguistics literature, in this paper we will focus on \emph{hyperbole}, \emph{irony}, and \emph{metaphor} as three of the most generally recognized and broadly studied figurative tropes. In what follows, we will describe each of the three tropes and review relevant theoretical and empirical research.
%%
\subsubsection{Hyperbole}
A hyperbole is an exaggerated statement that purposefully presents its subject as more striking or extreme than it actually is \cite{roberts1994people, mccarthy2004there}. \citeA{gibbs1994poetics} makes a distinction between hyperbole and overstatement, where the former is purposefully produced for rhetorical effect. Rhetoric studies in ancient Greece regarded hyperbole as a major figure of speech, often used to persuade and demonstrate power \cite{smith1969mystery}. Hyperbolic statements often include extreme case formulations (e.g. ``It was the biggest storm in the history of the universe'') or implausible descriptions (e.g. ``It's a thousand degrees outside.'') These demonstrations of non-veridicality require the listener to produce what \citeA{fogelin2011figuratively} called a ``corrective'' response that is more in line with reality.
 For a hyperbolic statement to be interpreted successfully, the listener must recognize the non-veridicality of the statement, thus entering an activity of joint pretense \cite{clark1996using}. 
 In a modern analysis of a corpus of spoken English, \cite{mccarthy2004there} found that hyperbole occurs frequently in everyday conversations and is often used in humorous and other affective contexts. 
 %\citeA{norrick1982semantics} proposed that hyperbole is characterized by three properties: its affective dimension, its pragmatic nature, and its function as a vertical-scale metaphor where the comparison is between different positions on a scale rather than between discrete concepts. 
  %
%\subsubsection{Understatement}
%An understatement is used to purposefully describe something as being less extreme than it actually is \cite{roberts1994people}: for example, saying ``It seems to be drizzling a bit'' in the middle of a storm. As is the case for hyperbole, the listener must also recognize the statement's non-veridicality and produce a corrective response. A hyperbole and an understatement are similar in the sense that both have literal meanings that contrast with reality in terms of degree \cite{mccarthy2004there}. In both cases, speaker and listener must have common knowledge about the state of the world to know that the statement is not literally true---e.g. it must be mutually believed that it is raining very hard in order for the listener to successfully interpret the understatement.

%
\subsubsection{Verbal irony}
An ironic statement describes something as contrary to what it actually is: for example, saying ``Such beautiful weather we are having'' in the middle of a storm \cite{roberts1994people, gibbs1999figurative}. 
%Irony is thought to be related to hyperbole because it also involves a vertical scale (niceness of the weather), where the literal meaning's position on the scale (``beautiful'') is  different from the position of the intended meaning (``terrible''). 
Like hyperbole, irony also requires the listener to recognize the non-veridicality of the utterance and enter into joint pretense. However, the required corrective response is one of ``kind'' (e.g. from ``beautiful'' to ``terrible'') instead of degree (e.g. from ``drizzling'' to ``pouring'') \cite{mccarthy2004there}. 
The listener and speaker must share sufficient common ground in order to ensure an appropriate corrective response. To explain how irony is identified and interpreted, \citeA{clark1984pretense} proposed the \emph{pretense theory of irony}, where the speaker is pretending to be an ignorant or injudicious person who believes the weather to be beautiful, thus expressing a derogatory attitude towards such a misinformed person.  
%say irony involves setting up a pretend world that is contrasted with the actual world to highlight the incongruity between what is and what might have been. 
\citeA{sperber1981irony}, on the other hand, proposed the \emph{echoic reminder theory}, which suggests that irony is used to remind listeners of jointly held beliefs, social norms, or expectations that are being disrespected. While the pretense and mention theories disagree some respects, they agree that the expression of attitude is central to the mechanism of verbal irony understanding \cite{sperber1984verbal}.
%Finally, it has been observed that ironic statements that are literally positive but express negative opinions (e.g. saying ``You're a great friend'' ironically to a friend who betrayed you) occur more frequently and are easier to comprehend than statements that are literally negative but express positive opinions (e.g. saying ``You're a terrible friend'' ironically to a friend who helped you). This phenomena, termed the asymmetry of affect, has been explained using pretense theory, 

%this asymmetry is due to the fact that ironic statements Since most social norms are positive, it follows naturally that ironic statements  Irony draws attention to this contrast and more often involves using a positive statement to express a negative attitude. 
% 
\subsubsection{Metaphor}
Metaphors are utterances that compare ideas or concepts from different domains in order to highlight commonalities.
For example, ``Juliet is the sun'' expresses Juliet's beauty and importance by comparing her to the sun, which also has many positive qualities.
% ``My lawyer is a shark'' communicates the lawyer's ruthlessness; and ``Art washes away from the soul the dust of everyday life'' allows Picasso to compare ``art'' to a cleansing fluid and ``the soul'' to a physical object that collects dust, which gracefully accomplishes two poetic metaphors at once. 
Metaphoric utterances are prevalent in both literary and everyday language \cite{gibbs1999figurative, roberts1994people}. 
%One can find traces of metaphoricity even in mundane utterances such as ``I waited for a long time,'' where the spatial term ``long'' is used to describe the abstract domain of time \cite{lakoff1993contemporary}. 
Due in part to its ubiquity and in part to the belief that metaphor is intimately tied to our ability to create mappings between concrete experiences and abstract concepts \cite{lakoff2008metaphors}, metaphor is by far the most widely studied trope in cognitive science and related fields \cite{gibbs2012interpreting}. Understanding a metaphor often requires identifying overlapping features or aligning analogical structures between two domains \cite{gentner1997alignment}. Using metaphors to understand a novel domain can significantly affect people's inferences, making it an important mechanism for learning and reasoning \cite{thibodeau2011metaphors, gentner1997reasoning}. %Evidence that metaphors are often processed as quickly as literal statements suggests that metaphor understanding does not require first accessing literal meanings   \cite{glucksberg2003psycholinguistics, gibbs2012interpreting}. 
From the communication perspective, researchers have suggested that metaphorical language may be a more economical and efficient way of expressing complex meanings  \cite{ortony1975metaphors, boerger2005variations,glucksberg1989metaphors}. As \citeA{glucksberg1990understanding} write, ``Metaphors are  used  to  communicate a complex, patterned  set of properties in a shorthand that is understood  by the  members of a speech community  who share relevant mutual  knowledge" (pp. 16). In communication tasks where pairs of participants are separated by a screen and asked to refer to abstract geometrical objects, participants often prefer to describe objects metaphorically in terms of other known objects rather than use literal analytical descriptions \cite{clark1986referring, glucksberg1989metaphors}. %Perhaps unsurprisingly, \citeA{fussell1989effects} found that these analogical and figurative descriptions tend to be shorter than literal descriptions. However, they also found that figurative descriptions are used significantly more often when the intended audience is one's self, where common ground is maximal, than when the intended audience is a different person. This suggests that people may be balancing efficiency and clarity when choosing figurative versus literal descriptions. 
A potential benefit of speaking in metaphor is thus to utilize common ground to communicate complex ideas more efficiently. %or necessarily involve different processing mechanisms from literal language  
%

%%
%\subsubsection{Indirect request}
%An indirect request is a command phrased as a comment or question \cite{roberts1994people}: for example, ``It is rather cold in here'' can be used as a request for the listener to close a window or turn down the air-conditioning. Indirect requests are often used to be polite \cite{roberts1994people}. However, highly indirect requests are not necessarily perceived as the most polite; instead, indirect requests that break away significantly from conventional forms are perceived as lacking pragmatic clarity and thus less polite \cite{blum1987indirectness}. Indirect requests can vary in terms of conventionality, where some utterances are interpreted as requests only in specific contexts (e.g. when the listener has control over the temperature in the room), whereas other utterances are highly conventionalized and almost always interpreted as requests (e.g. ``Can you pass the salt?''). \citeA{gibbs1983people} found that people do not compute the literal meanings of highly conventional indirect requests and instead are biased towards the figurative interpretations. \citeA{clark1979responding} proposed that when the answer to a question is mutually obvious to the speaker and listener, the listener should interpret the question as an indirect request instead of as a genuine question. 
%
%\subsubsection{Rhetorical question}
%%%
%A rhetorical question is a question that does not require an answer and is instead used for effect \cite{roberts1994people}: for example, ``How many times have I told you not to leave your dirty clothes on the floor?'' uttered by an upset parent. Rhetorical questions are often used to emphasize, express negative affect, and persuade \cite{roberts1994people}. If interpreted literally, rhetorical questions have answers that are either obvious (e.g ``Am I your maid?'') or nearly impossible to provide (e.g. ``How many times have I told you?''). If interpreted correctly, rhetorical questions effectively function as pseudo-assertions (e.g. ``I am not your maid'') as well as answers to their own questions (e.g. ``I have told you too many times'') \cite{schmidt1977so}. This requires the speaker and listener to have the mutual belief that the question does not require and in fact may not have an actual answer. It also requires the listener to go beyond the surface form of the question and infer that the speaker's true communicative intent is not to elicit an answer but rather to express a point.
%%%

%\subsection*{Effects of figurative language}
%
%%Figurative utterances have two distinct qualities of interest. First, their intended meanings . Second, they are often used with the intention to produce particular effects and in order to accomplish discourse goals beyond relaying objective information about the world.   
%
%In addition to differing in various ways from their literal or standard meanings, different types of figurative language share a distinctive quality: they are often used to accomplish goals beyond relaying objective information about the world and to produce rhetorical effects \cite{roberts1994people}.   
%%\cite{gibbs1999figurative, gibbs2012interpreting}
%% examined the discourse goals that motivate people to use various figurative tropes. They identified a taxonomy of goals, including to convey emotion, to emphasize, to be humorous, or to be eloquent, and find that metaphor
%%
%%These are, in order, to clarify ($82\%$), to add interest ($71\%$), to compare similarities ($35\%$), to provoke thought ($35\%$), and to be eloquent ($35\%$).
%%
%\citeA{colston1998you} showed that hyperbole and irony are often used to express surprise. Other work has shown that verbal irony can heighten or soften criticism \cite{colston1997salting}, elicit emotional reactions \cite{leggitt2000emotional}, highlight group membership \cite{gibbs2000irony}, and express affective attitudes \cite{roberts1994people}.
%% \cite{colston1998you} that hyperbole and irony are more often used with friends and may signal social intimacy between speaker and listener \cite{gibbs2000irony, pexman2004does, kreuz1996use}. 
%In addition, researchers have suggested that metaphors are often used to express subjective attitudes towards the subject \cite{ortony1979beyond}, and that subjective sentences frequently contain figures of speech such as metaphor and hyperbole \cite{riloff2005exploiting}. 
%%\todo{elaborate}
%It is possible that the intuitive judgment of figurativeness involves recognizing that the speaker's intent is not to communicate objective information about the world, but rather to produce one or more of these rhetorical effects. 
%As a result, it may be important to consider the affective subtexts and social information that figurative language communicates above and beyond most loose uses of language. 

\subsection*{Factors that shape figurative interpretation}
%It may be useful to further classify these eight types of figurative language in order to make the commonalities among them more salient. I will introduce the concepts of horizontal comparison, vertical comparison, and comparison of surface form. Metaphor and simile are horizontal comparisons in the sense that the concepts being compared (e.g. ``Juliet'' and ``the sun'') occupy different parts in a semantic space projected onto two dimensions. Hyperbole, understatement, and irony are vertical in the sense that the concepts being compared (e.g. ``drizzling'' / ``pouring'' ; ``lovely''/ ``terrible'') occupy similar points in the semantic space but vary vertically in intensity \cite{norrick1982semantics}. I will call indirect requests and rhetorical questions ``formal'' comparisons, where sentences in the form of questions or comments carry the same illocutionary force as sentences in the form of requests or declarative statements.

In reviewing these three figurative tropes, some common features emerge. First, each example from these three tropes produces multiple interpretations that are distinct and highly different from each other (e.g. \emph{it's literally a thousand degrees outside} v.s. \emph{it's unexpectedly hot outside, like 90 degrees}; \emph{the weather is amazing} v.s. \emph{the weather is terrible};  \emph{Juliet is made of hot plasma}  v.s. \emph{Juliet is beautiful}). Second, the intended meanings of these utterances are related to their ``literal'' meanings in non-arbitrary ways (e.g. a thousand degrees and 90 degrees are both unexpectedly high; ``beautiful'' and ``terrible'' both describe an extreme attitude towards the weather; the sun and Juliet are both very important and appealing to Romeo).
%Third, these utterances tend to express speakers' subjective experiences and attitudes rather than objective information about the world. 
Finally, a great deal of common ground is required to successfully interpret these utterances. 
%For example, interpretation of an utterance such as ``Such beautiful weather we are having,'' depends upon the speaker and listener's mutual beliefs about the relevant state of the world (e.g. it is raining), their shared background knowledge (e.g. sunshine is usually preferable to rain), and mutual awareness of potential discourse goals (e.g. the speaker wants to convey her opinion about the weather). Because the interpretation of such utterances depends upon these different flavors of common ground, it tends to be highly sensitive to changes in context. 
In what follows, we will examine in more detail the various factors that shape figurative interpretation.

%For example, suppose a chemist needs to make a chemical solution using water that is $100$ degrees celsius. She asks her lab technician, ``How's the temperature?'' and he replies, ``The water is boiling.'' The chemist will interpret this utterance to mean that the water is at boiling point. Now suppose a mother is about to give her baby a bath and asks her husband to test the temperature of the bathwater. He replies, ``The water is boiling.'' Given the mother assumes the utterance to be relevant to the topic and specific task at hand, she will interpret the utterance to mean that the water is too hot, but unlikely that it is $100$ degrees celsius, thus arriving at a figurative, hyperbolic interpretation. Now suppose Sam says, out of the blue, ``Bob is boiling.'' Without knowing what entity Bob is or the context in which Sam produced the utterance, it is difficult to arrive at an interpretation of the utterance.  If what is relevant is Bob's temperature, this utterance means that Bob has a fever. If the topic is Bob's emotional state, this means that Bob is angry. A relevance theory account of metaphor is that all meanings, not just figurative, figurative ones, are selected based on relevance to the topic at hand.

%%%
%\section{What is figurative language?}


%However, most theories of figurative language are informal and not fully precise, leaving room for discrepancies in how researchers interpret empirical data. %TODO: references. 
%Furthermore, different theories focus on disparate aspects of figurative language (e.g. the standard pragmatic view focuses on recognition; the structure-alignment model focuses on interpretation of relational metaphors, etc.), making it difficult to compare predictions about the same phenomenon. Figurative language understanding involves many moving pieces---literal meaning, common ground, contextual information, speaker intention, feature attribution, subjective attitude and affect, etc. A model that explicitly describes how all of the pieces interact may help resolve some of the discrepancies among theories and provide a unified framework for examining various types of figurative language. In particular, several important questions are left unanswered by the theories reviewed above. I will explore a few of the questions in this section and introduce a formal computational framework in Chapter 3 that may shed light on the answers.
%In this section, we first review some classic ideas in pragmatics. We then describe two main bodies of work that specifically examine the pragmatics of figurative language understanding. Finally, we suggest that a complete theory of figurative communication should more carefully account for certain important factors that affect general language understanding.

%Finally, the interpretations of these utterances are highly context dependent. I argue that all of these different flavors of information will be important when we consider how rich interpretations emerge from figurative utterances.
%
%
%These three types of language use all exhibit three distinct qualities that we are particularly interested in: (1) the intended meanings of the utterances differ (often dramatically) from the literal meanings (2) the utterances often express speakers' subjective experiences and attitudes rather than objective information about the world (3) the interpretations of these utterances are highly context dependent. 

\subsubsection*{Literal meaning}
Figurative utterances communicate meanings that differ dramatically from their literal meanings. However, these intended, figurative meanings are still related to the literal meanings in a non-arbitrary manner \cite{coulson2005blending}. One cannot simply say \emph{any} sentence and expect the context to make one's intended meaning clear---for example, saying ``I had eggs for breakfast today'' in the middle of a storm will unlikely be successful at communicating that the weather is terrible. Instead, an utterance such as ``Such beautiful weather we're having'' can be interpreted as expressing a negative attitude towards the weather because its literal meaning draws attention to the weather as well as the speaker's attitude towards it. As a result, the literal semantics of utterances should play a role in determining the figurative meanings that people derive, and a model of figurative interpretation should be able to represent and make use of an utterance's literal semantics.  
%important to represent the literal semantics of utterances and consider its role in deriving figurative meanings.
%The puzzle, then is \emph{how} the intended meaning of a figurative utterance could be derived from its literal semantics. 
%reasonable to assume that the intended meaning of a figurative utterance can be derived from its literal meaning. 

%important to consider the various ways in which we can formulate literal meaning and analyze the ways in which it contributes to figurative meaning.

%The traditional view of literal meaning draws a clear distinction between literal and figurative meanings. 
%According to \citeA{frege1984sense}, the literal meaning of a sentence is its interpretation given no information about who said the sentence, when, or why. This view led to assumptions about the literal meaning of a sentence as being determined only by the meanings of its component words and how they are composed, independent of any extra-linguistic information. As a response against this traditional view, \citeA{searle1978literal} argued that literal meaning is not entirely independent of extra-linguistic information and instead relies heavily on background knowledge. For example, the literal meanings of ``Sally cut the cake'' and ``Sally cut the grass'' depend on the manners in which cake and grass are usually cut, which is encoded in background knowledge \cite{gibbs1984literal}. Searle draws a distinction between this kind of background knowledge, which he believes is needed to determine literal meanings, and the context in which sentences are uttered, which helps determine contextual meanings. However, other linguists and philosophers argue that Searle ``demands too much from literal meaning'' and conflates the literal meaning of a sentence with its intended speaker meaning \cite{dascal1981contextualism, katz1981literal, gibbs1984literal}. 
%
%Another way literal meaning has been conceptualized is through conventionality \cite{davies1996philosophy}. Perhaps literal meanings are those that are more conventional, such that without additional context, one chooses the sentence's most conventional meaning by default \cite{recanati2002literal}. However, equating literalness with conventionality is problematic when we consider idioms. For example, the conventional meaning of ``kick the bucket'' (to die) is certainly different from our intuitive sense of its ``literal'' meaning (to come into contact with a bucket with one's foot). \citeA{giora1997understanding} proposed that instead of focusing on the literal/figurative distinction, a more useful dimension along which to analyze meaning is salience. She introduces the graded salience hypothesis, where more salient meanings---defined as more frequent, context-independent, and prominent---are accessed first and are ``default,'' regardless of whether or not they are literal. While the idea of salience many explain when and why certain meanings are accessed automatically independent of context and literalness \cite{giora1999priority}, it is not always clear precisely how salience should be measured or operationalized \cite{gibbs2012interpreting}.
%Despite vibrant discussions of these and related matters in the past few decades, the problem of literal meaning is still largely unresolved \cite{recanati2004literal, cruse2004meaning, carston2008linguistic, coulson2005blending}

%\todo{add a note here about how the literal meanings we're looking at are rather simple and uncontroversial?}
%This exploration of various views on literal meaning suggests that the problem is highly complex, and a thorough and satisfying account is possibly beyond the scope of this document. 

\subsubsection*{Encyclopedic knowledge}
One way in which literal meaning gives rise to intended meaning is through encyclopedic knowledge, 
which goes beyond the strict literal meanings of utterances to include stereotypes, conventions, and a community's beliefs and practices \cite{taylor2003linguistic, langacker1987foundations}. 
%, known as an encyclopedic approach to meaning
%One of the most important insights in the study of language use is that interlocutors make heavy use of common ground during communication \cite{clark1996using}. 
%This common ground includes the extra-linguistic information and world knowledge shared among interlocutors, which can be intimately tied to linguistic meaning. 
%Some researchers propose that the meaning of a word itself includes encyclopedic knowledge. 
%\citeA{searle1978literal} argued that literal meaning is not entirely independent of extra-linguistic information and instead relies heavily on encyclopedic knowledge. For example, the literal meanings of ``Sally cut the cake'' and ``Sally cut the grass'' depend on the manners in which cake and grass are usually cut, which is encoded in background encyclopedic knowledge \cite{gibbs1984literal}. %Searle draws a distinction between this kind of background knowledge, which he believes is needed to determine literal meanings, and the context in which sentences are uttered, which helps determine contextual meanings. 
%However, other linguists and philosophers argue that Searle ``demands too much from literal meaning'' and conflates the literal meaning of a sentence with its intended speaker meaning \cite{dascal1981contextualism, katz1981literal, gibbs1984literal}. 
%
%Despite the fact that the distinction between literal meaning and encyclopedic knowledge is not always clear, 
This extralinguistic information plays an important role in the interpretation of language, including figurative uses. For example, suppose Ann asks Bob, ``Is Cam an honest person?'' and Bob replies, ``He's a politician.'' Although Bob's answer is indirect, Ann will likely interpret it to mean that Bob does not believe Cam to be an honest person. This interpretation arises because while the dictionary meaning of ``politician'' is  \emph{a person who is professionally involved in politics}, the encyclopedic meaning of the word can encompass many more features and connotations, such as \emph{dishonest} and \emph{corrupt}. Bob's utterance not only asserts Cam's profession (the literal, dictionary meaning of ``politician''), but also attributes features associated with that profession to Cam. 
Ann is able to successfully interpret  Bob's utterance, and Bob is able to successfully use this utterance, because they both have access to the relevant encyclopedic meaning of ``politician.''
%---the network of background knowledge shared among people in a community, which includes the stereotype that politicians may be dishonest or corrupt \cite{taylor2003linguistic, langacker1987foundations}. 
Naturally, Ann's interpretation is sensitive to the contents of the background knowledge they share. If Ann and Bob belong to a community where politicians are associated with honesty, then Ann would interpret Bob's reply to mean that he believes Cam is an honest person. 
%Similarly, ``It's a thousand degrees outside'' is interpreted as ``It's unbearably hot outside'' partly based on the encyclopedic knowledge that ``a thousand degrees'' is exceedingly hot, and that one is unlikely to survive under that temperature. 
As a result, the encyclopedic knowledge that interlocutors share can heavily influence the interpretation of figurative utterances.

\subsubsection*{Prior beliefs}
In addition to encyclopedic knowledge, language interpretation depends upon the listener's prior beliefs and expectations about various states of the world. \citeA{hormann1983tun} showed that people's interpretation of quantifiers such as ``several'' and ``few'' vary based on the kinds of objects to which they refer. For example, ``several crumbs'' is interpreted to mean \emph{around ten crumbs}, while ``several mountains'' is interpreted to mean \emph{around five mountains}. \citeA{clark1991words} explains this phenomenon using the ``principle of possibilities:'' to interpret language, people use their prior expectations about which situations or worlds are possible as well as the likelihood of those worlds. 
%To interpret ``several crumbs'' and ``several mountains,'' people consider the number of crumbs and mountains that typically inhabit a scene or situation. 
Since a typical situation involving crumbs is likely to contain more crumbs than a typical situation involving mountains, the interpretation of ``several'' is higher in ``several crumbs''  than in ``several mountains.''

%\jtk{describe example with ``it's a million degrees outside''}

Given that prior beliefs affect the interpretation of superficially straightforward terms such as ``several,'' it is unsurprising that prior beliefs factor into the interpretation of figurative language as well. In the example dialogue with Ann and Bob, Ann's interpretation of the utterance ``He's a politician'' is sensitive to her prior beliefs about Cam. Suppose Ann does not know what Cam does for a living. She will have learned two facts about Cam from Bob's utterance: Cam is a politician, and Cam is not an honest person. Suppose, on the other hand, Ann knows that Cam is a professional politician. She will not have learned anything new about Cam's profession from Bob; however, even though she knows that politicians are commonly believed to be dishonest, Bob's utterance makes her more certain that Bob thinks Cam \emph{in particular} is dishonest, because that is the most informative and relevant interpretation given her question. Finally, suppose Ann knows that Cam is not professionally involved in politics at all. Instead of updating her beliefs about Cam using the dictionary meaning of ``politician,'' she will rely on its encyclopedic meaning to conclude that Cam is dishonest (but not professionally involved in politics), resulting in a metaphorical interpretation. These examples show that interpretation of the same utterance in the same local context can vary in a rich and subtle manner based on the speaker and listener's prior beliefs and expectations about the topic. 

\subsubsection*{Local context}

A great deal of psycholinguistics research has shown that figurative language is highly sensitive to the preceding context, affecting both the speed and product of interpretation \cite{katz2001moment, giora2003our, coulson2005blending}. 
%\todo{more examples from the literature?} 
If Ann had asked, ``Is Cam a persuasive speaker?'' instead of ``Is Cam an honest person?'' then Bob's utterance would be interpreted as a compliment about Cam's eloquence\footnote{Note that in this case Bob's utterance still carries the connotation that Cam is not to be trusted, even though Ann's did not explicitly ask about Cam's honesty.}.
In the pragmatics literature, context has been construed as information that specifies the topic of conversation. \citeA{roberts1996information} refers to this topic as the ``question under discussion'' (hereafter QUD), and argues that utterances are expected to be relevant to the QUD and are interpreted with respect to it. 
%The QUD can be determined by an explicit question, for example Ann's question about Cam's honesty, which guides her interpretation of Bob's response because she expects Bob to communicate information that is relevant to her question.  
Often, the QUD does not take the form of an explicit question and is not clearly specified to the listener. In this case, the listener must jointly infer the QUD as well as the speaker's intended meaning given an utterance.
Given the importance of local context in shaping interpretation, a model of figurative language understanding should flexibly integrate this type of contextual information via inferences about the QUD.
%A speaker may produce an utterance in order to introduce a new QUD, which the listener must then infer based on the utterance itself as well as her expectations about which QUDs the speaker may wish to introduce. As a result, part of the listener's task in language comprehension is to jointly infer the QUD and the utterance meaning given the meaning.
%that a speaker's utterance addresses
%The local context---whether constructed by an explicit question, a situation, or a salient goal---sets up a ``question under discussion'' (QUD) to which the information conveyed by the speaker is intended to be maximally relevant. In Liz and Sam's original dialogue, the QUD was Bob's honesty; in this scenario, it is Bob's persuasiveness; in the balcony scene of \emph{Romeo and Juliet,} it is Juliet's admirable qualities. 


\subsubsection*{Pragmatic reasoning}
A critical insight in communication is that a speaker does not produce utterances in a social vacuum; he considers the listener's beliefs, goals, and disposition in order to determine which utterance is most effective in a given situation  \cite{clark1982audience}. In turn, a listener considers the speaker's beliefs, goals, and disposition (as well as the speaker's representation of the listener's beliefs, goals, and disposition) in order to select the most likely meaning of an utterance \cite{clark1996using, levinson2000presumptive, grice20134}. 
%This recursive social reasoning between speaker and listener is responsible for many phenomena in pragmatics and language understanding \cite{horn2006implicature, levinson2000presumptive}.
%Furthermore, listeners assume speakers to be rational and cooperative agents who aim to be informative, known as the Cooperative Principle \cite{grice20134, clark1996using, levinson2000presumptive}. When interpreting an utterance, a listener uses these assumptions of rationality and informativeness to reason about what meaning a speaker could want to convey that would lead him to choose a particular utterance.
This recursive social reasoning between listener and speaker is responsible for many phenomena in pragmatics and language understanding \cite{horn2006implicature, levinson2000presumptive}.

In addition, listeners can make many powerful inferences about utterances by representing speakers as rational and intentional agents who choose utterances in order to accomplish a specific communicative goal. 
Consider again the conversation between Ann and Bob. Ann has several hypotheses about Bob's communicative goal and the QUD his utterance aims to address, which could be Cam's honesty, profession, or persuasiveness as a speaker. The likelihood of each QUD depends on the context (e.g. Ann's question) and the beliefs shared by speaker and listener (e.g. Ann's prior knowledge of Cam's profession). 
%Ann's question (the context) and as well as  and  which is likely given  Ann's question. His goal could be to inform Ann of Cam's profession, which is likely if Ann does not know Cam's profession, but less likely if Cam's profession is in common ground. 
Given each QUD, Ann can make inferences about what information Bob intends for her to glean from his utterance. 
In addition to depending on the QUD, the array of implicatures derived from an utterance is also affected by the alternative utterances that the speaker could have said \cite{horn2006implicature, bergen2012s}. The fact that Bob could have said ``Yes, he's a persuasive speaker'' but chose to say ``He's a politician'' makes it likely that Bob wants to address QUDs that go beyond Cam's persuasiveness. %Furthermore, the fact that Cam chose the metaphor ``He's a politician'' instead of ``He's a salesman,'' both of which convey persuasiveness, suggests that Bob wants to communicate specific features about Cam such as deceptiveness and cunning, rather than pushiness. 
Reasoning about the speaker's choice of utterance and available alternatives allows the listener to use basic principles of communication derive rich figurative meanings as well as their subtleties. A theory of figurative language as a communicative act should thus incorporate the speaker's intent as well as how the listener reasons about this intent in various communicative contexts.

\subsubsection{Putting it all together}
%My review of the literature has revealed three general principles that are needed to explain figurative language understanding. 
%First, listeners reason about speakers and assume that they are rational and cooperative agents who wish to be informative. Second, speaker and listener use their shared background knowledge (common ground) to communicate, which includes specific knowledge of certain states of the world as well as general encyclopedic knowledge. Third, listeners are sensitive to the question under discussion (QUD) created by the local context and assume that speakers choose utterances that are relevant to the QUD. 
While researchers have suggested that the construction of meaning involves an interplay of the components outlined above \cite{coulson2005blending, gibbs1984literal, clark1996using, stalnaker2002common}, to our knowledge there is no formal model that explicitly describes the relationships among these components and integrates them to produce quantitative and fine-grained predictions that can be evaluated against empirical data. 
%\todo{elaborate on why it's useful to have a formal model}
Such a formal model would be useful for clarifying specific assumptions in the theory, testing the detailed effects of inputs such as encyclopedic knowledge and prior beliefs, and comparing alternative explanations \cite{frank2013throwing}.
Here we propose a computational framework in the tradition of Rational Speech-Acts models, which formalizes pragmatic reasoning and the recursive nature of communication \cite{frank2012predicting}. In addition to pragmatic reasoning, we incorporate the other factors described above to capture figurative interpretation. We show that these components together produce appropriate interpretations of figurative utterances as well as rich affective and social subtexts. 
%\todo{add note that these components are not unique to figurative language understanding}
%the following components: reasoning about the speaker's choice of utterance, with assumptions of rationality and informativeness; the literal meaning of utterances; shared background knowledge between speaker and listener; specific prior beliefs; local contextual information; affective subtext. 


%A listener who reasons about a speaker can make many more powerful inferences about an utterance by thinking about \emph{why} he chose a particular utterance and what communicative goal it satisfies \cite{gibbs2012interpreting}. 
%It seems plausible that listeners also need to reason about the speakers' intent in order to interpret figurative utterances in various communicative contexts.

\section{Probabilistic Models of Language Understanding}

%In the last chapter, I proposed that in order to successfully interpret a figurative utterance, a listener needs to (1) reason about a rational and informative speaker (2) consider the literal meaning of an utterance 
%(3) consider relevant background knowledge associated with utterance
%(4) consider relevant prior beliefs 
%(5) consider the local context and question under discussion 
%(6) understand the subtext or attitudes expressed by the utterance. 
%Here I will review the basic RSA framework and introduce an extension that incorporates these components.
%listeners reason about speakers and assume that they are rational and cooperative agents who wish to be informative. Second, speaker and listener use their shared background knowledge (common ground) to communicate, which includes specific knowledge of certain states of the world as well as general encyclopedic knowledge. Third, listeners are sensitive to the question under discussion (QUD) created by the local context and assume that speakers choose utterances that are relevant to the QUD.

%\subsection{Basic Rational Speech Act Model}

In recent years, a family of computational models have emerged that use probabilistic tools to formalize principles of communication, called Rational Speech-Acts (RSA) models \cite{frank2012predicting, goodman2013knowledge, goodman2014probabilistic}. 
These models formalize the Cooperative Principle to explain how people arrive at pragmatically enriched meanings of utterances through recursive social reasoning. By representing listeners as agents who reason about the intentions of a rational and cooperative speaker, these models predict pragmatic enrichments that allow the listener to make inferences beyond the strict literal meaning of an utterance. To date, RSA models have been used to explain Horn implicatures \cite{bergen2012s}, vagueness and context-sensitivity in gradable adjectives \cite{lassiter2014context} and generics \cite{tesslergeneric}, the pragmatic use and interpretation of prosody \cite{bergen2015strategic}, and more \cite{goodman2014probabilistic}. 
%\section{The RSA framework}
%This is accomplished 
% A recent body of work called Rational Speech Act (RSA) theory formalizes the first principle regarding informativity and rationality \cite{goodman2014probabilistic, frank2012predicting, goodman2013knowledge}. RSA theory uses probabilistic models to 
%through the recursive reasoning between speaker and listener, which allows the listener to reason about what intentions a rational and cooperative speaker could have had to lead him to choose a particular utterance. 

The basic structure of RSA models is simple and usually involves three ``agents:'' a naive literal listener $L_0$, a speaker $S_1$, and a sophisticated, pragmatic listener $L_1$. $S_1$ reasons about $L_0$ and determines which utterance $u$ to choose in order to efficiently communicate a meaning $m$ to $L_0$. The more sophisticated listener $L_1$ then reasons about which meaning $m$ most likely led $S_1$ to choose $u$ and uses Bayesian inference to recover $m$ given $u$, thus capturing the recursive reasoning between speaker and listener.

Formally, $L_0$'s interpretation of an utterance $u$ is given by the following equation:

\begin{equation}
L_0(m | u) = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if $m$ = $\llbracket u\rrbracket$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
%
The probability that $S_1$ will choose an utterance $u$ given an intended meaning $m$ is given by the following equation:
%Suppose Sam (to be more ``formal,'' we will call him $S_1$) intends to communicate meaning $m$. He reasons about Liz (we will call her $L_0$) and considers how likely it is that she will infer $m$ from $u$. 
\begin{equation}
S_1 (u|m) \propto e^{\lambda U(u | m)} 
\end{equation}
%
where $U(u | m)$ is a utility function that specifies the negative surprisal of the meaning $m$ under $L_0$'s interpretation (i.e. how likely $L_0$ will receive the meaning $m$ given utterance $u$), and $\lambda$ is a rationality parameter that determines $S_1$'s tendency to choose utterances that have maximal utility.
%informativeness, or negative surprisal the utility of where $L_0 (m|u)$ is the probability that $L_0$ will arrive at meaning $m$ given utterance $u$, and $Cost(u)$ is the psychological cost of producing utterance $u$ given its length, difficulty, or availability. 
The term $e^{U(u | m)}$ thus implements the Luce-choice rule \cite{luce2005individual}, which is widely used to model rational decision-making.

Using Bayes' rule to infer $S_1$'s intended meaning given a generative model of $S_1$'s utterance choice,  $L_1$'s interpretation distribution of $u$ is given by the following equation:

\begin{equation}
L_1 (m|u) \propto P(m)S_1 (u|m)
\end{equation}
where $P(m)$ is the prior probability of the meaning $m$, or how likely it is that meaning $m$ is true. $L_1$ is thus used to model people's pragmatic interpretation of utterances\footnote{In principle, the speaker and listener can recursively reason about each other to arbitrary depth. However, rich pragmatic effects can emerge from depths 1 and 2, which is reason to believe that this framework may be psychologically plausible for modeling pragmatic language understanding.}.
%It is important to note, however, that these models operate on the computational level of Marr's levels of analysis \cite{marr1982vision} and do not make specific claims about the particular algorithms employed in language processing.

\citeA{frank2012predicting} tested the RSA framework on humans' pragmatic judgments in a simple reference game. In this paradigm, participants see three objects and are asked to choose which object the speaker is referring to (see Figure 3.1). The speaker can only use one word to identify the intended object, which often results in ambiguous references. For example, the word ``blue'' may refer to either the blue square or the blue circle in Figure 3.1. \citeA{frank2012predicting} asked participants how likely a speaker is to use a particular word to refer to an object: for example, how likely a speaker is to use the word ``blue'' or ``square'' to refer to the blue square. This experiment yields the likelihood term $S_1(u|m)$. Other participants were asked how likely a speaker is to refer to a particular object using an unknown word, which measures $P(m)$, or what the authors refer to as the object's contextual salience. Using these two pieces of information, the RSA model computes $L_1(m|u)$, which is the probability that the referent is a particular object given a particular utterance. The model  correctly predicts that listeners are more likely to judge the word ``blue'' as referring to the blue square, even though the word is technically ambiguous. This is because a sophisticated listener who reasons about the speaker knows that if the speaker had meant the blue circle, he would have used ``circle'' instead because it is more informative. The model's predictions matched participants' judgments extremely well ($r = 0.99, p < 0.0001$), suggesting that people may be incorporating the speaker's choices and prior probabilities of meanings in a similar rational manner. Using a simple reference game paradigm, this work showed that incorporating recursive social reasoning and prior knowledge allows the listener to go beyond the strict literal meaning of a word to infer the intended meaning in context.

%\begin{figure}[t]
%\centering
%\includegraphics[width=8.7cm]{frankgoodman2012.png}
%%\includegraphics[width=8.7cm]{model_effects.pdf}
%\caption{Stimuli from \citeA{frank2012predicting}}
%\label{frankgoodman}
%\end{figure}

\citeA{goodman2013knowledge} made more explicit the fact that in addition to formalizing the rationality principle, the RSA model can also flexibly capture background knowledge and common ground. Imagine Bob has three apples, which Ann cannot see. Bob says, ``Some of the apples are red.'' Ann makes the inference that \emph{not all} of the apples are red, because if all of the apples are red, then Bob would have said ``All of the apples are red'' in order to be maximally informative. The pragmatic strengthening of ``some'' to ``some but not all''---termed scalar implicature---can arise based on the same principles that allow a listener to infer \emph{blue square} from  ``blue'' in Figure 3.1 \cite{frank2012predicting}. However, what happens when the speaker and listener both know that the speaker's knowledge of the world is incomplete? Suppose Bob can only see two of the three apples. To choose an utterance that is maximally informative, Bob needs to consider the possible states of the world and compute the expected utility of different utterances. His choice of utterance is captured with this equation:
\begin{equation}
S_1 (u | s, a) = \sum_o S_1(u | o, a) P(o | a, s) 
\end{equation}
where $u$ is the utterance, $s$ is the true state of the three apples, $a$ is Bob's perceptual access to the three apples, and $o$ is what he observed. Given that Ann knows Bob's perceptual access to the apples, (i.e.  $a$ is common knowledge between Ann and Bob), her inference is captured by the following:
\begin{equation}
L_1(s | u, a) \propto S_1(u | s, a)P(s)
\end{equation}
This model closely matches participants' interpretations of utterances given different combinations of observations and perceptual access ($r = 0.96$). This result suggests that by explicitly incorporating common ground about what the speaker knows and does not know, listeners can interpret utterances in principled ways even when the speaker has imperfect knowledge of the world.

While the RSA framework provides an intuitive and empirically validated way to model the interaction between literal meaning and background knowledge, it requires significant and theoretically important extensions to explain figurative communication.
%RSA models have been shown to be extremely flexible and productive for modeling language understanding. 
In most of the cases that RSA handles, the pragmatically strengthened interpretations produced by $L_1$ do not stray very far from the literal meanings of utterances. While interpreting ``blue'' to mean \emph{blue square} requires pragmatic enrichment, the interpreted meaning is simply more specific than the literal meaning, and not distinct from the literal meaning as is the case in many figurative uses. 
One of the key assumptions in the basic RSA model is that $S_1$ chooses an utterance that most efficiently communicates the intended meaning to $L_0$. 
Since $L_0$ interprets utterances literally, in this basic setup it is never optimal for $S_1$ to choose an utterance whose literal meaning directly contradicts the intended meaning.
%However, this type of contradictory use often characterizes figurative language.
For example, suppose $S_1$ wants to communicate that the weather is \emph{terrible}. According to the basic RSA model, $S_1$ reasons about the literal listener $L_0$ to choose the utterance that will most likely convey this information. 
Because $L_0$ is a literal listener, she would interpret the utterance ``The weather is amazing'' to mean that $S_1$ believes the weather is literally \emph{amazing}. She would thus \emph{not} arrive at the interpretation that the speaker believes the weather is \emph{terrible}. 
As a result, $S_1$ has no reason to say ``The weather is amazing'' to communicate that the weather is \emph{terrible} (because $L_0$ would not receive the intended meaning). Consequently, a pragmatic listener who reasons about why the speaker chose various utterances will not interpret ``The weather is amazing'' to mean that the weather is \emph{terrible}. The fact that speakers in the basic RSA model is restricted to communicating about the state of the world (in this case, the weather) means that it is unable to explain many cases of figurative language use, because the literal listener would not be able to recover that information. However, intuition as well as prior research suggests that in many cases of language use, the speaker's goal is to communicate not the full world state, but rather a subset or specific dimension of the world state. We explain this intuition and our formalization of it in more detail in the following section.


%\section{Extensions to RSA}
%We extend the RSA framework specifically to address the ways in which general background knowledge and relevance to the question under discussion shape language understanding. 
%\subsection{Reasoning about dimensions of meaning}
%The basic RSA models already incorporate some degree of background knowledge through prior beliefs. For example, to compute the probability that Bob is a wolf given the utterance ``Bob is a wolf,'' the pragmatic listener must consider the prior probability that Bob is a wolf. However, believing that Bob is a wolf is more than believing that Bob is a large wild animal that often hunts in groups. Once you believe that Bob is a wolf, you are more likely to believe that Bob is furry, fierce, loyal, fast, hungry ... any number of things that you associate with wolves. These beliefs are graded; you might have a strong belief that any given wolf is fierce, but only a weak belief that any given wolf is loyal.  This network of background knowledge forms a rich multi-dimensional representation of what it means to be a wolf. Note that while these other dimensions of meaning may not be part of the core ``literal'' meaning of the word ``wolf,'' they are associative and easily accessible. %TODO: cite
%As a result, we assume that the basic-level literal listener has access to these dimensions of meaning. The literal listener's interpretation is now given by the following: 
%
%\begin{equation}
%L_0(m_0, \vec {m} |u) = \left\{ 
%  \begin{array}{l l}
%    P(\vec {m} | m_0) & \quad \text{if $m_0$ = $u$}\\
%    0 & \quad \text{otherwise}
%  \end{array} \right.
%\end{equation}
%%
%where $m_0$ is the core literal meaning, and $\vec {m}$ is a vector of various associated encyclopedic meanings. This provides a formal way to enrich literal encoded meaning with associated background knowledge. However, introducing multiple dimensions of meaning alone is insufficient for explaining figurative language understanding. While the literal listener has access to the associated meanings, she still interprets utterances literally. Given the utterance ``Bob is a wolf,'' the literal listener will believe that Bob is a fierce, furry, and loyal wolf with some probability ($P(\vec m | m_0)$); however, she does \emph{not} believe that Bob is a fierce person or any kind of person at all, because she believes that he is a wolf with $100\%$ probability.  
%
%\subsection{Reasoning about relevance}
%
%For figurative meaning to arise, the speaker and pragmatic listener must reason about which dimension of meaning is relevant to the question under discussion. We formalize the principle of relevance by introducing a function $Q$, which projects the meaning that a literal listener derives from an utterance onto only the relevant dimension. This leads to the following utility function for speaker $S_1$:
%\begin{equation}
%U(u | m_0, \vec {m}, Q) = \log \sum_{m_0,\vec {m}'} \delta_{Q(\vec m)=Q(\vec m')} L_0(m_0, \vec m' |u)
%\end{equation}
%%
%Given this utility function, the speaker's choice of utterance is the following:
%\begin{equation}
%S_1(u | m_0, \vec m, Q) \propto e^{\lambda U(u | m_0, \vec m, Q)},
%\end{equation}
%where $\lambda$ is an optimality parameter \cite{luce2005individual}. 
%%
%The pragmatic listener $L_1$ then performs Bayesian inference to guess the intended meaning given prior knowledge and her internal model of the speaker. Since she is uncertain about the precise question under discussion, she marginalizes over the possible QUDs under consideration:
%$$
%L_1 (m_0, \vec m | u) \propto P(m_0) P(\vec m | m_0) \sum_{Q}{P (Q) S_1 (u|m_0, \vec m, Q)}
%$$
%This equation now includes multiple dimensions of meaning, the QUD, a model of the speaker's choice given he wants to be relevant and informative, and the listener's prior beliefs. Something quite magical happens when all of these elements are combined. Since the literal listener is likely to believe that Bob is fierce if she believes that Bob is a wolf, the speaker is motivated to say ``Bob is a wolf'' to get her to believe that Bob is a wolf and thus fierce. Furthermore, since the speaker only cares to communicate Bob's fierceness and not which species Bob belongs to, he does not mind that the literal listener will believe that Bob is actually a wolf. The pragmatic listener knows this about the speaker and also knows that Bob is very unlikely to actually be a wolf. Combining these pieces of information, the pragmatic listener ultimately believes that Bob is a fierce person, which is the intuitive interpretation of the sentence ``Bob is a wolf.'' By incorporating QUD inference, the model allows local context to constrain and shape the dimensions of meaning that the listener derives from an utterance. 
%
%Rational Speech Act (RSA) models are a family of probabilistic models that formalize the Cooperative Principle to model how people arrive at pragmatically enriched meanings of utterances \cite{goodman2014probabilistic, frank2012predicting, goodman2013knowledge}. Under the RSA framework, speaker and listener recursively reason about each other to communicate. A speaker reasons about how to get a particular meaning across to a naive listener; a more sophisticated listener then reasons about the speaker and uses Bayesian inference to recover the intended meaning.
%
%Suppose a speaker $S_1$ intends to communicate meaning $m$. He reasons about a naive listener $L_0$ and considers how likely it is that she will infer $m$ from $u$. We assume that $S_1$ incurs a ``cost'' by uttering $u$, which is proportional to the physical or cognitive effort required to produce $u$. In accordance with many models of decision-making, we compute the probability that $S_1$ will choose utterance $u$ given meaning $m$ and cost $c$ using the Luce choice rule \cite{luce2005individual}:
%\begin{equation}
%S_1 (u|m) \propto L_{0} (m|u) \cdot e^{-Cost(u)}
%\end{equation}
%%
%However, a real listener does not naively interpret utterances in a social vacuum; she considers \emph{why} the speaker chose utterance $u$ to communicate meaning $m$. This more sophisticated listener, whom we shall call $L_1$, uses Bayes' Rule to infer $m$ based on her model of how $S_1$ chooses his utterances. This is captured by the following equation:
%\begin{equation}
%L_1 (m|u) \propto P(m)S_1 (u|m)
%\end{equation}
%where $P(m)$ is the prior probability of the meaning $m$. This allows $L_1$ to incorporate background knowledge of $m$ in her interpretation. In principle, the speaker and listener can recursively reason about each other to an arbitrary depth. However, rich pragmatic effects can emerge from depths 1 and 2, which is reason to believe that this framework may be psychologically plausible for modeling pragmatic language understanding. 

\subsection{Rational Speech Act Model with QUD inference}

We extend the RSA framework to accommodate the idea that speakers may only want to communicate specific dimensions of the world that are relevant to the conversation at hand, or to the QUD. We show that this extension enables literal meaning, encyclopedic knowledge, prior beliefs, and contextual information to work together to produce figurative interpretations. 
%shape language understanding through reasoning about relevance to the QUD.
%question under discussion, or QUD \cite{roberts1996information}. A QUD picks out an immediate topic under discussion, which the speaker is likely to address in order to maintain discourse coherence and achieve the goals of the conversation. Note that the QUD does not have to be established by an explicit question, although it can be; instead, a question under discussion is more generally a contextual element that ``proffers a set of relevant alternatives which the interlocutors
%commit themselves to addressing'' \cite{roberts1996information}. In other words, a QUD helps determine the speaker's  communicate goal by constraining the set of things that the speaker may want to address.

The basic RSA models already naturally incorporate aspects of background knowledge and prior beliefs. For example, consider the utterance: ``Cam is a wolf.'' To compute the probability that Cam is a wolf given this utterance, the pragmatic listener considers the prior probability of Cam being a wolf. On the other hand, believing that Cam is a wolf is more than believing that Cam is a large wild animal that often hunts in groups. Once you believe that Cam is a wolf, you are more likely to believe that Cam is furry, fierce, loyal, fast, hungry, etc. These beliefs are graded; one may have a strong belief that any given wolf is fierce, but only a weak belief that any given wolf is loyal.  This network of encyclopedic knowledge forms a rich multi-dimensional representation of what it means to be a wolf. Note that while these other dimensions of meaning may not be part of the core ``literal'' meaning of the word ``wolf,'' they are easily accessible through association and are closely tied to the literal meaning. As a result, we assume that the literal listener $L_0$ also has access to these dimensions of meaning. 

We now revise the literal listener's interpretation given the assumption that she has access to these encyclopedic meanings. Given a world state $s$, associated encyclopedic meanings $\vec{A}$, and an utterance $u$, the $L_0$'s interpretation of $u$ is now given by the following: 

\begin{equation}
L_0(s, \vec {A} | u) = \left\{ 
  \begin{array}{l l}
    P(\vec {A} | s) & \quad \text{if $s$ = $\llbracket u\rrbracket$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
%
We thus provide a formal way of enriching literal meaning with encyclopedic knowledge. However, incorporating encyclopedic knowledge alone is insufficient for explaining figurative language understanding. Although the literal listener now has access to the associated encyclopedic meanings, she still assigns $0$ probability to all interpretations that are incompatible with the literal meaning of the utterance. Given the utterance ``Bob is a wolf,'' the literal listener will believe that Bob is a fierce, furry, and loyal wolf with some probability $P(\vec A | s)$; however, she does \emph{not} believe that Bob is a fierce person or any kind of person at all, because she believes that he is a wolf with probability $1$.

For figurative meaning to arise, the speaker and pragmatic listener must reason about the QUD and which dimension of meaning is relevant to it.
We formalize relevance to the QUD by introducing a function $Q$, which projects the meaning that a literal listener derives from an utterance onto only the dimension that is under discussion. In other words, the speaker does not care about whether the literal listener derives true information regarding any of the other dimensions; the utility of an utterance is determined only by whether information along the QUD dimension is communicated  to the literal listener. This leads to the following revised utility function for speaker $S_1$:
\begin{equation}
U(u | s, \vec {A}, Q) = \log \sum_{s', \vec {A}'} \delta_{Q(s, \vec A)=Q(s', \vec A')} L_0(s', \vec A' |u)
\end{equation}
%
Based on this utility function, the speaker's choice of utterance is specified by the following:
\begin{equation}
S_1(u | s, \vec A, Q) \propto e^{\lambda U(u | s, \vec A, Q)},
\end{equation}
%where $\lambda$ is a rationality parameter that determines the speaker's tendency to choose the optimally informative utterance \cite{luce2005individual}. 
%
Consistent with the basic RSA models, the pragmatic listener $L_1$ performs Bayesian inference to guess the intended meaning given prior knowledge and her internal model of the speaker. Since $L_1$ is uncertain about the precise QUD that the speaker is trying to address, she marginalizes over the possible QUDs under consideration:
\begin{equation}
L_1 (s, \vec A | u) \propto P(s) P(\vec A | s) \sum_{Q}{P (Q) S_1 (u|s, \vec A, Q)}
\label{L1}
\end{equation}
This equation now includes prior beliefs about likely world states $P(s)$, encyclopedic knowledge associated with those world states $P(\vec A | s)$, the QUD $Q$, and a model of the speaker's choice of utterance given that he wants to be informative and relevant to the QUD. 

Something quite magical happens when all of these elements are combined. We illustrate this with the example utterance ``Cam is a wolf'' and a set of QUDs that includes Cam's personality characteristics. Since the literal listener is likely to believe that Cam is fierce if she believes that Cam is a wolf, the speaker is motivated to say ``Cam is a wolf'' to get her to believe that Cam is a wolf and thus fierce. A speaker who only cares to communicate Cam's fierceness (the QUD dimension) and not which species Cam belongs to will not mind that the literal listener will believe that Cam is actually a wolf. The pragmatic listener then simulates the speaker's choice of utterance given different QUDs. Combining this simulation with the prior belief that Cam is very unlikely to actually be a wolf, the pragmatic listener ultimately believes that Cam is a fierce person, which is the intuitive interpretation of the utterance ``Bob is a wolf.'' 
This simple example suggests that by incorporating QUD inference with encyclopedic knowledge, the pragmatic listener modeled by RSA is free to produce interpretations of utterances that are not literally true. We will refer to this RSA model extended to include QUD inference as the qRSA model.
%In particular, we will show that the model captures several desired effects in the interpretation of \emph{hyperbole}, \emph{irony}, and \emph{metaphor}: (1) figurative interpretation (2) sensitivity to encyclopedic knowledge (3) sensitivity to prior beliefs (4) sensitivity to utterance cost (5) sensitivity to local context (6) sensitivity to alternative utterances. %\todo{think about order of these effects} 

\section{Modeling Figurative Language}
We describe three domains in which we empirically tested qRSA model: \emph{hyperbole}, \emph{irony}, and \emph{metaphor}. Through experiments and model comparison, we show that the qRSA model captures several desired effects in figurative interpretation.

Our first attempt to test the qRSA model on figurative language focused on cases where the literal semantics are simple to quantify and relatively uncontroversial: number words. Although numbers have precise meanings in mathematics, they are interpreted in various nonliteral ways in natural language. For example, ``It's 90 degrees'' is likely to be interpreted as meaning it's approximately $90$ degrees, while ``It's 92 degrees'' is more likely to be interpreted as exactly $92$ degrees. This tendency to expand the interpretation of round numbers to include neighboring numbers is known as pragmatic halo \cite{lasersohn1999pragmatic}. 
Even more dramatically, an utterance such as ``It's 1000 degrees outside'' is likely to receive a hyperbolic interpretation: it is very hot outside, but the temperature is much less than 1000 degrees.
%Here we describe behavioral experiments and specific implementations of the qRSA model that demonstrate interesting pragmatic effects in hyperbole, irony, and metaphor. The work on hyperbole has been previously published in \citeA{kao2014figurative} and will only be briefly summarized; the experiments and models for irony and metaphor will be described in detail.
%
%\subsection{Hyperbole}
%%A hyperbole is an exaggerated statement that purposefully presents its subject as more extreme than it actually is \cite{roberts1994people, mccarthy2004there}. 
%%%Rhetoric studies in ancient Greece regarded hyperbole as a major figure of speech, often used to persuade and demonstrate power \cite{smith1969mystery}. 
%%In a modern analysis of a corpus of spoken English, \cite{mccarthy2004there} found that hyperbole occurs frequently in everyday conversations and is often used to express emotions and provoke humorous responses. 
%%\citeA{norrick1982semantics} proposed that hyperbole is characterized by three properties: its affective dimension, its pragmatic nature, and its function as a vertical-scale metaphor where the comparison is between different positions on a scale rather than between discrete concepts. For a hyperbolic statement to be interpreted successfully, the listener must recognize the non-veridicality of the statement, thus entering an activity of joint pretense \cite{clark1996using}.  Hyperbolic statements often include extreme case formulations (e.g. ``It was the biggest storm in the history of the universe'') or implausible descriptions (e.g. ``It was a thousand degrees outside.'') These demonstrations of non-veridicality then require the listener to produce what \citeA{fogelin2011figuratively} called a ``corrective'' response that is more in line with reality.
%Our first attempt , we focus on the nonliteral interpretation of number words. We chose number words be- cause they have precise literal meanings that can be easily modeled, and apply to domains (such as prices) that lend themselves to quantitative measurement. 
%
%\begin{figure}[t]
%\centering
%\scalebox{0.33}{\includegraphics{Plots/figure2a-revised.pdf}}
%\caption{Model predictions v.s. average human interpretations. Each point represents an utterance and price state pair ($u, s$). The x-coordinate of each point is the probability of the model interpreting utterance $u$ as meaning price state $s$; the y-coordinate is the empirical probability. Correlation between model and human interpretations is $0.968$.}
%\label{model_fit}
%\end{figure}
%
%\begin{figure}[t]
%\centering
%\scalebox{0.33}{\includegraphics{Plots/figure4a-revised.pdf}}
%\caption{Model predictions of affect v.s. human ratings. Each point represents an utterance and price state pair $(u, s)$. For pairs where $u = s$, the utterance is literal; for $u > s$, the utterance is hyperbolic. The x-coordinate of each point is the model's prediction of the probability that the utterance/price state pair conveys affect; the y-coordinate is participants' affect ratings (error bars are standard error). Correlation between model and humans is $0.775$.}
%\label{affect}
%\end{figure}
%
%
In \citeA{kao2014nonliteral}, we examined how people arrive at the appropriate interpretations and affective subtexts of numeric utterances, specifically in the domain of prices.
%We conducted experiments using the prices of three everyday items---electric kettles, watches, and laptops. 
To examine the effect of prior beliefs on nonliteral interpretation, 
we asked participants to rate the probabilities that different items (electric kettles, watches, and laptops) cost various amounts of money (e.g. \$50, \$51, \$1,000, \$10,000). To measure people's encyclopedic knowledge about prices, we asked participants to rate the probability that someone would think an item that costs \$$x$ is expensive (e.g., a watch that costs \$1,000), thus explicitly introducing subjective attitudes (judgments of expensiveness) as a dimension of meaning.
%We chose expensiveness as the associated dimension of interest, because utterances about cost seem to naturally evoke judgments of expensiveness \todo{explain this more}.   
Using the empirically measured prior beliefs and encyclopedic knowledge, the qRSA model produced interpretations for each utterance. The model reasons about different types of QUDs that the speaker may wish to address, including the exact price of the item, the approximate price, and the speaker's subjective judgment about the price. 
%By assuming that the speaker may wish to address QUDs beyond the exact price of the item, the model produces nonliteral interpretations and captures both pragmatic halo and hyperbole. 
To interpret an utterance such as ``The laptop cost 1000 dollars,'' the model reasons as follows: the price could have been \$ 999, and the speaker may have chosen to say ``1000 dollars'' because he only cares to communicate the approximate price to the listener, and ``a thousand'' is more efficient to produce than ``nine hundred and ninety nine.''  To interpret an utterance such as ``The electric kettle cost 1000 dollars,'' the model then reasons that a kettle is highly unlikely to cost \$1000, and that the speaker may have chosen to say ``1000 dollars'' because he only cares to communicate his judgment about the price, and a literal listener is most likely to infer \emph{expensive} from an extremely high price. %saying an extremely high number is the most likely to communicate ``e t prior probabilities of watches are a basic feature of hyperbole: utterances whose literal meanings are less likely given the price prior are more likely to be interpreted hyperbolically. For example, ``The watch cost 1000 dollars'' is more likely to be interpreted hyperbolically than ``The  laptop cost 1000 dollars.'' 

To quantitatively evaluate the model's predictions, we asked participants to interpret various numeric utterances (e.g., Sam said: ``The electric kettle cost 1000 dollars.'' How likely is it that the watch cost $x$ dollars?) For all utterances, we compared the model's predictions with participants' responses, and found that they are highly correlated ($r=0.968, p<0.0001$). %(Figure \ref{model_fit}), 
This result suggests that the qRSA model is able to combine linguistic information, background knowledge, and reasoning about the speaker's goals to produce nonliteral, hyperbolic interpretations. 

%\begin{figure}
%\centering
%\scalebox{0.4}{
%\includegraphics{figure2b-revised.pdf}}
%\caption{Comparison of models with different communicative goals and human interpretations for the utterance ``The electric kettle cost 1,000 dollars.'' A model that considers both affect and precision goals (full model) most closely matches human data.}
%\label{goals}
%\end{figure}

In addition to producing the appropriate corrective response to hyperbolic utterances, the model also captures the affective subtext of hyperbole. We conducted a separate experiment to examine peoples' interpretation of affect in hyperbolic versus literal utterances. Participants read scenarios in which a speaker bought an item that cost $s$ dollars and says it cost $u$ dollars, where $u \geq s$. They then rated how likely it is that the buyer thinks the item was too expensive. Results showed that utterances $u$ where $u > s$ (hyperbolic utterances) are rated as significantly more likely to convey affect than utterances where $u {=} s$ (literal utterances) ($F(1, 25)=12.57, p < 0.005$). Moreover, if a watch actually cost $100$ dollars and Sam produces a hyperbolic utterance such as ``The watch cost $1000$ dollars,'' participants are more likely to believe that Sam thinks the watch is expensive than if the watch \emph{actually} cost $1000$ dollars and Sam produces an identical (but in this case literal) utterance: ``The watch cost $1000$ dollars.'' This suggests that listeners infer affect from hyperbolic utterances above and beyond the affect associated  with a given price state. Quantitatively, we compared model and human interpretations of affect for each of the 45 utterance and price state pairs $(u, s)$ where $u \geq s$. While there is a significant amount of noise in the human judgments (average split-half correlation is $0.833$), the model predicts human interpretations of the utterances' affective subtext significantly better than chance ($r=0.775, p < 0.00001$), capturing most of the reliable variation in these data. %(Figure \ref{affect}).
 
Results from \cite{kao2014nonliteral} suggest that by incorporating inferences about the speaker's communicative goals, the qRSA model successfully interprets hyperbolic utterances and recovers the affective subtext. 
However, in this initial exploration of applying the qRSA model to figurative language, we only considered a very simple kind of encyclopedic knowledge, namely how likely it is for a state of the world (a price) to be associated with a negative feeling (expensiveness).
This simplification overlooks the range of attitudes and emotions that speakers could express using figurative utterances. 
In the next section, we explore how expanding the space of encyclopedic knowledge to include emotions with positive/negative valence and high/low arousal accounts for people's interpretations of ironic utterances.
%The model's quantitative predictions closely match humans' judgments on hyperbole, a complex phenomenon previously beyond the scope of computational models.

%\todo[inline]{Propose that the same formal architecture may work for other uses of figurative language that are quite different}
\subsection{Verbal Irony}
%\todo[inline]{Introducing a richer model of affect gives irony}
An ironic statement describes something as contrary to what it actually is \cite{roberts1994people, gibbs1999figurative}. For example, a speaker who says ``Such beautiful weather we are having'' in the middle of a storm means that the weather is \emph{not} beautiful and expresses a negative attitude towards it. %Like hyperbole, verbal irony also requires the listener to recognize the non-veridicality of the utterance and enter into joint pretense. However, the required corrective response is one of ``kind'' (e.g. from ``lovely'' to ``terrible'') instead of degree (e.g. from ``pouring'' to ``drizzling'') \cite{mccarthy2004there}. 
How do people appropriately interpret these superficially positive or negative utterances? Can our model use QUD inference to interpret an utterance when its literal meaning is not just an exaggerated version of the intended meaning, but rather its opposite?
%\cite{kao2014figurative} showed that this model---which we will refer to as qRSA---produces figurative interpretations of hyperbolic utterances that closely match humans'; however, they considered only a simplified affect space, namely the presence or absence of negative feeling. This overlooks the range of attitudes and emotions that speakers could express with figurative utterances. In particular, since verbal irony involves expressing negative meanings with positive utterances and vice versa, a richer space of affect that includes both positive and negative emotions may be key. 
In this section, we will examine the consequence of expanding the set of encyclopedic knowledge we consider to an empirically derived affect space. We show that this minimal change enables the qRSA model to capture many of the rich inferences resulting from verbal irony.

In what follows, we will examine interpretations of potentially ironic utterances in an innocuous domain---the weather. We chose the weather as the victim of irony for several reasons. First, people are quite familiar with talking (and complaining) about the weather.
%, and remarks about the weather have often been used as examples in previous discussions of verbal irony\todo{cite}. 
Second, we can visually represent the weather to participants with minimal linguistic description in order to obtain measures of nonlinguistic contextual knowledge. For example, we can show participants a picture of a blue, cloudless sky and ask them to judge how likely it is that someone would perceive the weather to be \emph{amazing} or \emph{terrible}. Finally, given the critical role that prior beliefs play in understanding irony, we can vary the probability of different beliefs by manipulating the context (e.g. showing a picture of a gray, cloudy sky instead of blue sky) to observe how the same utterance is interpreted differently given different prior expectations. 
%This offers to our knowledge the first fine-grained manipulation and quantitative measure of context in studies of irony. 

In what follows, we first explore how an enriched space of encyclopedic knowledge affects the qRSA model and show that it produces ironic interpretations in a simple simulation. We then present two behavioral experiments that examine people's interpretations of utterances given different weather contexts. We show that by including two affective dimensions in our encyclopedic knowledge---valence and arousal---our model produces interpretations that closely match humans'. 
%Finally, we discuss implications of our model for informal theories of irony and its relationship to other types of figurative language understanding.

\subsubsection{Model}

%In this section, we revisit the qRSA model and compare different spaces of affect to test the conditions for producing ironic interpretations. 

Following the qRSA model described previously, a speaker chooses an utterance that most effectively communicates information regarding the QUD to a literal listener. We consider a meaning space that consists of the variables $s, \vec A$, where $s$ is the state of the world, and $\vec A$ represents the speaker's (potentially multidimensional) affect towards the state. 
Following the formulation described in the modeling section, we formalize a QUD as a projection from the full meaning space to the subset of interest to the speaker, which could be $s$ or any of the dimensions of $\vec A$. 
We specify the speaker's utility as information gained by the listener about the topic of interest---the negative surprisal of the true state under the listener's distribution given an utterance, $u$, along the QUD dimension, $q$. 
%This leads to the following utility function: 
%\begin{equation}
%U(u | s, A, Q) = \log \sum_{s', A'} \delta_{Q(s, A)=Q(s', A')} L_{0}(s', A' |u)
%\end{equation}
%where $L_{0}$ describes the literal listener, who updates her prior beliefs about $s, A$ by assuming the utterance to be true of $s$. 
%The speaker's choice of utterance $u$ given state $s$, his affect $A$ towards the state, and the QUD is then described by the following:
%$S_1(u | s, A, Q) \propto e^{\lambda U(u | s, A, Q)}$,
%where $\lambda$ is the rationality parameter.
%%
A pragmatic listener $L_{1}$ takes into account prior knowledge and his internal model of the speaker to determine the state of the world as well as the speaker's affect, marginalizing over the possible QUDs under consideration (see Equation~\ref{L1}).
%$$
%L_{1}(s, A | u) \propto P(s) P(A | s) \sum_{q}{P (q) S (u|s, A, q)}
%$$
%
We characterize the interpretation of an utterance as the resulting posterior distribution over world states and speaker affects, $L_1(s, \vec A | u)$.
\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Plots/sim12.pdf}}
\caption{Model interpretations of ``The weather is terrible'' given different prior beliefs about the weather state and affect dimensions. Gray dotted lines indicate prior beliefs about weather states given a weather context; blue lines indicate interpretations when reasoning only about the speaker's valence; orange lines indicate interpretations when reasoning about both valence and arousal.}
\label{sim12}
\end{figure}

We performed the following simulations to examine the model's behavior using affect spaces, $\vec A$, that differ in complexity and structure.  
We assume that the state of the weather $s$ can be one of possible ordered values: \textit{terrible}, \textit{bad}, \textit{neutral}, \textit{good}, and \textit{amazing}. We consider two different weather contexts: apparently bad weather and apparently amazing weather, which are each specified by a prior distribution over these states (see gray dotted lines in Figure~\ref{sim12}). We then examine how the model interprets the sentence ``The weather is terrible'' in each of the two weather contexts, given different affect spaces.

We first consider a one-dimensional affect space, where the dimension is emotional valence, and the values are whether the speaker feels negative or positive valence towards the state.  
The blue lines in Figure~\ref{sim12} show the model's interpretation of ``The weather is terrible'' using this one-dimensional affect space. 
The model is capable of non-literal interpretation: it produces a hyperbolic interpretation (that the weather is merely \textit{bad}) given ``The weather is terrible'' in the bad weather situation. However, it produces a literal interpretation (that the weather is \textit{terrible}) in the amazing weather situation. This is because a pragmatic listener who only considers emotional valence does not believe that the speaker has any reason to choose a negative utterance to express positive affect (because the utterance communicates no true information). As a result, a pragmatic listener that only considers one dimension of affect--emotional valence---is unlikely to infer a positive world state from a negative utterance (and vice versa), thus failing to evidence verbal irony. 

%We now consider a more complex affect space with two dimensions---valence and arousal---to observe its consequence on interpretation. 
This model simulation reveals a critical puzzle in the interpretation of verbal irony. What true information \emph{could} a speaker communicate about a positive world state using a negative utterance? Affective science identifies two dimensions, termed valence and arousal, that underly the slew of emotions people experience \cite{russell1980circumplex}. 
For example, \emph{anger} is a negative valence and high arousal emotion, while \emph{contentment} is a positive valence and low arousal emotion. 
%These two dimensions are not independent, with low arousal implying more neutral valence. 
%\todo{JTK: is this non-independence important/necessary to note, or is it confusing?}
%\todo{NDG: is this dependence built into the model's prior? if so, does it matter? if not, we should explore it...} 
Could speakers leverage the arousal dimension to convey high arousal and positive affect (e.g. excitement) using utterances whose literal meanings are associated with high arousal but negative affect (e.g. ``The weather is terrible!'')? We test the consequences of incorporating a dimension of emotional arousal in the space of affects a listener considers. The orange lines in Figure~\ref{sim12} show simulations of the qRSA model with a two-dimensional affect space: whether the speaker feels negative/positive valence and low/high arousal towards the weather state. Given strong prior belief that the weather state is \textit{bad}, the model interprets ``The weather is terrible'' to mean that the weather is likely to be \textit{bad}, again producing a hyperbolic interpretation. However, given strong prior belief that the weather is \textit{amazing}, the model now places much greater probability on the ironic interpretation of ``The weather is terrible,'' meaning that the weather is likely \textit{amazing}. This is because, with the enriched two-dimensional affect space, the pragmatic listener realizes that the speaker may be using ``terrible'' to communicate high emotional arousal. 
%Note that this result is not simply due to the model falling back on the prior: given the same priors, the model interprets the neutral utterance ``The weather is ok'' as the weather state being \textit{neutral} and not \textit{amazing}.
These simulations suggest that a more psychologically realistic, two-dimensional affect space enables the qRSA model to interpret ironic utterances in addition to hyperbolic ones. 


%\todo{NDG: perhaps we could have marker size in fig 1 correspond to interpreted arousal? never mind, we don't have that for the first sim...}

%\begin{figure}
%    %\centering
%    %\begin{minipage}{0.45\textwidth}
%        %\centering
%        \includegraphics[width=320pt, height=250pt]{Plots/image-grid}
%        \caption{Weather images shown to participants in Experiments 1 and 2.}
%        \label{images}
%    %\end{minipage}
%\end{figure}
    %\begin{minipage}{0.45\textwidth}
       % \centering
  \begin{figure}
        \includegraphics[width=400pt, height=300pt]{Plots/irony-state-prior-withpic.pdf}
        \caption{Smoothed prior probability distributions over weather states for each of the nine weather contexts. Participants saw each image and chose a state label from the set: \textit{terrible, bad, neutral, good, amazing}. Probability distributions over weather states were computed by performing Laplace smoothing on the counts for each state label given a weather context and normalizing the counts to sum up to $1$.  
     %Each panel represents a weather context; each line represents the distribution over weather states for a weather context prior to any linguistic input. For example, the vast majority of participants rated weather context 1 as \emph{amazing}, suggesting that the prior probability of someone believing weather context 1 to be \emph{amazing} is extremely high.
     }
        \label{irony-priors}
    %\end{minipage}
    %\caption{Nine weather contexts and their empirically measured priors over weather states.}
    %\label{fig:three graphs}
\end{figure}
To quantitatively test whether the qRSA model with expanded affect space can capture a range of ironic interpretations, we need appropriate prior distributions as well as data for human interpretations.
We conducted Experiment 1a to measure prior beliefs over weather states ($P(s)$) for a range of weather contexts, as well as the likelihood of various emotions towards each weather state. Information about emotions associated with each weather state allows us to empirically derive the affective space and priors, $P(A | s)$, for this domain.
In Experiment 1b, we collected people's ratings of how a speaker perceives and feels about the weather given what she says in a weather context (e.g. ``The weather is terrible!'' when the context clearly depicts sunny weather).

\subsubsection{Experiment 1a: Background knowledge for verbal irony}


\begin{figure}
        \includegraphics[width=340pt, height=260pt]{Plots/irony-biplot-labeled.pdf}
        \caption{Biplot of the first two principle components of the seven emotion ratings. The first two PCs correspond roughly to valence and arousal, with positively valenced emotions (\textit{excited, happy, content})clustering on the right, and more high arousal emotions (\textit{disgusted, excited}) appearing appearing at the top.}
        \label{irony-pca}
\end{figure}
    
\begin{figure}
    %\begin{minipage}{0.45\textwidth}
       \includegraphics[width=380pt, height=200pt]{Plots/irony-affect-prior.pdf}
       \caption{Average probabilities of positive valence and high arousal given each weather state. Error bars are $95\%$ confidence intervals. Probability of positive valence increases monotonically over the five weather states; probability of high arousal follows a symmetric U-shaped curve and does not differ significantly for the \textit{terrible} and \textit{amazing} weather states.}        
        \label{irony-affect-prior}
    %\end{minipage}
    %\caption{Nine weather contexts and their empirically measured priors over weather states.}
    %\label{fig:three graphs}
\end{figure}

%\subsubsection{Experiment 1a: Prior elicitation}
%textbf{Materials and methods}\\
We selected nine images from Google Images that depict the weather. To cover a range of weather states, three of the images were of sunny weather, three of cloudy weather, and three of rainy or snowy weather. Each of these images represent what we will call a ``weather context,'' as shown in Figure~\ref{irony-priors}.

$49$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant saw all nine images in random order. In each trial, participants were told that a person (e.g.~Ann) looks out the window and sees the view depicted by the image. They then indicated how Ann would rate the weather using a labeled 5-point scale, ranging from \textit{terrible}, \textit{bad}, \textit{neutral}, \textit{good}, to \textit{amazing}. Participants also used slider bars (end points labeled ``Impossible'' and ``Absolutely certain'') to rate how likely Ann is to feel each of the following seven emotions about the weather: \emph{excited}, \emph{happy}, \emph{content}, \emph{neutral}, \emph{sad}, \emph{disgusted}, and \emph{angry}, which are common emotion categories \cite{ekman1992argument}\footnote{From the most frequently cited set of six basic emotions, we removed \emph{fear} and \emph{surprise} and added \emph{content} and \emph{excited} to have a balanced set of positive and negative emotions. We also added \emph{neutral} to span a wider range of emotional arousal.}.
The order of the emotions was randomized for each participant but remained consistent across trials\footnote{Link to Experiment 1a: \url{http://stanford.edu/~justinek/irony_exp/priors/priors.html}}.
%\textit{Results}

For each of the nine weather contexts, we obtained the number of participants who gave each of the weather state ratings. We performed add-one Laplace smoothing on the counts to compute a smoothed prior distribution over weather states given each context, namely $P(s)$ (Figure~\ref{irony-priors}).
% shows that the sunny and positive weather contexts were more likely to be rated as \texttt{amazing}, while the negative weather contexts were more likely to be rated as \texttt{bad} or \texttt{terrible}.  %\todo[inline]{RDH: obviously, need some way of identifying what the different rows and columns of the grid mean}
To examine participants' ratings of the affect associated with each context, we first performed Principal Component Analysis (PCA) on the seven emotion category ratings. This allowed us to compress the ratings onto a lower-dimensional space and reveal the main affective dimensions that are important in this domain, as is often done in studies of emotion ratings \cite{russell1980circumplex}. We found that the first two principal components corresponded to the dimensions of emotional valence and emotional arousal, accounting for $69.14\%$ and $13.86\%$ of the variance in the data, respectively. As Figure~\ref{irony-pca} shows, the first two principle components successfully distinguish positively valenced emotions (\emph{excited, happy, content} from negatively valenced emotions (\emph{disgusted, angry, sad)}, as well as high arousal emotions (\emph{excited, disgusted}) from low arousal emotions (\emph{content, neutral, sad}). 
%\todo{we should do a free response version of this task (``Ann feels ..... about the weather.''), before and after a statement, at some point to see if we are missing any affects... probably not needed for cogsci.}

The PCA represents emotion ratings for each trial as real values between negative and positive infinity on each of the dimensions. To map these values onto probability space, we first standardized the scores on each dimension to have zero mean and unit variance. We then used the cumulative distribution function to convert the standardized scores into values between $0$ and $1$. 
This gives us the probabilities of Ann feeling positive (vs.~negative) valence and high (vs.~low) arousal for each trial, which is a two-dimensional probabilistic representation of her affect.
%Although these values are treated as probabilities of a binary variable, they approximately track the \emph{degree} of positive valence and arousal as well; for example, $P(A_v) = 0.6$ suggests that the valence is more positive than neutral, and $P(A_a) = 0.1$ suggests that the arousal is low. 
By calculating the average probabilities of positive valence and high arousal given each weather state rating, we obtain the probability of positive valence and high arousal associated with each weather state, namely $P(A | s)$ (Figure ~\ref{irony-affect-prior}). We observe that the probability of positive valence given a weather state increases monotonically across the ordered set of states: \emph{terrible, bad, neutral, good}, and \emph{amazing}, where the probability of positive valence given a \emph{terrible} state is significantly lower than the probability given an \emph{amazing} state. However, the probability of high arousal given each weather state follows a U-shape curve, where the probability of high arousal given a \emph{terrible} state is approximately equivalent to the probability of high arousal given an \emph{amazing} state. In other words, while the valences associated with \emph{terrible} and \emph{amazing} differ significantly, the arousals evoked by these states are actually very similar.
%\todo[inline]{NDG: does this imply that valence and arousal are independent? they generally aren't.... i'm a bit confused about where the model's $A$ states fall with respect to the valence-arousal quadratic curve....}

%\todo{JTK: something about how this is not a perfect transformation but does approximately the right thing} 

%\begin{figure}
%\begin{minipage}[b]{.5\textwidth}
%\includegraphics[width=5cm, height=3.7cm]{priors.pdf}
%\end{minipage}
%\begin{minipage}[b]{.4\textwidth}
%\includegraphics[width=5cm, height=5cm]{biplot.pdf}
%\end{minipage}
%
%\end{figure}

\begin{figure*}[t]
\scalebox{0.5}{\includegraphics{Plots/irony-state-human-model-withpics.pdf}}
\caption{Model's and participants' inferences about the weather state (x-axis) given a weather context (column) and an utterance (row). Each panel represents an interpretation given an utterance in a weather context. The dark lines are participants' ratings; the light lines are the model's posterior distributions over weather states.}
\label{irony-model-state}
\end{figure*}

\begin{figure*}[t]
\scalebox{0.5}{\includegraphics{Plots/irony-affect-human-model-withpics.pdf}}
\caption{Model's and participants' inferences about the probability of valence and arousal (row) given a weather context (column) and an utterance (x-axis). The dark lines are participants' ratings; the light lines are the model's posterior probabilities of positive valence and high arousal given an utterance in a weather context. The dotted lines are prior probabilities of positive valence and high arousal for each weather context. Error bars are $95\%$ confidence intervals on the participants' ratings.}
\label{irony-model-affect}
\end{figure*}
\todo{Is this figure useful?}
\begin{figure*}
    %\begin{minipage}{0.45\textwidth}
       \scalebox{0.55}{\includegraphics{Plots/irony-goal-model.pdf}}
       \caption{Model's posterior distributions over QUDs given an utterance (row) in a weather context (column). The darkness of the bars indicate participants' irony ratings for the utterances. }        
        \label{irony-goal-model}
    %\end{minipage}
    %\caption{Nine weather contexts and their empirically measured priors over weather states.}
    %\label{fig:three graphs}
\end{figure*}

\subsubsection{Experiment 1b: Interpreting verbal irony}
%In Experiment 1, we obtained the prior distribution over weather states for each weather context as well as the prior probabilities of positive valence and high arousal given each weather state. 
%Results from Experiment 1b give us the components to generate interpretations of utterances from our model. 
We conducted Experiment 1b to elicit people's interpretations of utterances, which we then use to evaluate model predictions. 
%\textbf{Materials and methods}\\
$59$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant saw all nine images from Figure~\ref{images} in random order. In each trial, participants were told that a person (e.g.~Ann) and her friend are in a room looking out the window together and see the view depicted by the image. Ann says, ``The weather is \underline{\hspace{1cm}}!'' where the adjective is randomly selected at each trial from the following set: ``terrible,'' ``bad,'' ``ok,'' ``good,'' and ``amazing.'' Participants first rated how likely it is that Ann's statement is ironic using a slider with end points labeled ``Definitely NOT ironic'' and ``Definitely ironic.'' They then indicated how Ann would actually rate the weather using a labeled 5-point scale, ranging from \textit{terrible}, \textit{bad}, \textit{neutral}, \textit{good}, to \textit{amazing}. Finally, participants used sliders to rate how likely Ann is to feel each of seven emotions about the weather \footnote{Link to Experiment 1b: \url{http://stanford.edu/~justinek/irony_exp/interpretation/interpretation_askIrony.html}}. 

%\textbf{Results}\\
We first examined participants' irony ratings for each of the weather context and utterance pairs. We found a basic irony effect, where utterances whose polarities are inconsistent with the polarity of the weather context are rated as significantly more ironic than utterances whose polarities are consistent with the weather context ($t(34.16)= -11.12, p < 0.0001$). For example, ``The weather is terrible'' (a negative utterance) is rated as more ironic in Weather Context 1 (positive context) ($M = 0.90$, $SD = 0.21$) than in Weather Context 7 (negative context) ($M =0.15$,	$SD = 0.27$). A linear regression model with the polarity of the utterance, the polarity of the weather context, and their interaction as predictors of irony ratings produced an adjusted $R^{2}$ of 0.91, capturing most of the variance in the data. This suggests that participants' lay judgments of irony align with its basic definition: utterances whose apparent meanings are opposite in polarity to the speaker's intended meaning.

Given that participants can identify verbal irony based on its inconsistency with context, how do they then use context to determine the speaker's intended meaning? We examined participants' interpretations of utterances given different contexts. For each of the $45$ weather context (9) $\times$ utterance (5) pairs, we obtained the number of participants who gave each of the five weather state ratings (\textit{terrible, bad, neutral, good, amazing}). We performed add-one Laplace smoothing on the counts to obtain a smoothed distribution over weather states given each context and utterance (dark lines in Figure~\ref{irony-model-state}). Results show that participants produce ironic interpretations of utterances, such that the weather is most likely to be \textit{amazing} given that the speaker said ``The weather is terrible'' in Weather Context 1. Participants also produce hyperbolic interpretations, such that the weather is most likely to be \textit{bad} given that the speaker said ``The weather is terrible'' in Weather Context 7. This confirms the intuition that people are highly sensitive to their contextually determined prior beliefs, and use these beliefs both to determine when an utterance is not meant literally and to appropriately recover the intended meaning. 

Finally, we examine participants' inferences about the speaker's affect given utterances in context. We used the loadings from the PCA on emotion ratings from Experiment 1a to project the emotion ratings from Experiment 1b onto the same dimensions. We then standardized and converted the scores into values between $0$ and $1$, as before, which gives us probability ratings of the speaker feeling positive valence and high arousal given an utterance and weather context.
We visualize the valence and arousal expressed by different utterances in different contexts using Figure ~\ref{irony-model-affect}. The dotted gray lines are the average prior probabilities of positive valence and high arousal associated with each weather context (without any linguistic input), taken from Experiment 1a. 
The dark solid lines show participants' average probability ratings of valence and arousal given an utterance in a weather context. We observe that participants' valence and arousal ratings vary systematically based on both the utterance the weather context. %are a function of both the utterance and the weather context. 
%For the \emph{positive} weather contexts, the speaker is interpreted as more likely expressing positive valence given the extremely negative utterance ``terrible'' than given the less negative utterance ``bad.'' On the other hand, for the \emph{negative} weather contexts, the speaker is interpreted as less likely expressing positive valence given the extremely positive utterance ``amazing'' than the less positive utterance ``good'' (STATS). This suggests that extreme utterances that are inconsistent with the valence of the context are more likely to express an opposite affect than its literal meaning.
%\todo[inline]{maybe you could use different names for the weather contexts? Like $\mathcal{W}_{neg}, \mathcal{W}_{neut}, \mathcal{W}_{pos}$, and then refer to $w_i \in \mathcal{W}_{neg}$ and so on.}

%Figure ~\ref{arousal} shows the average probability of \emph{high arousal} given an utterance in a weather context. The dotted gray lines are the average probabilities of high arousal associated with each weather context without any linguistic input, taken from Experiment 1. 
%We see that regardless of the weather context, more extreme utterances (e.g. ``terrible'' and ``amazing'') are more likely to communicate high arousal.
%\todo[inline]{JTK: figure out data take-home point here}
%\todo{it's interesting that ``terrible'' seems to have suppressed arousal compared to ``bad''. any thoughts about why?}


%We call this distribution the \emph{interpreted meaning} of an utterance in context. We found that participants often rated the speaker (e.g. Ann) as judging the actual weather state to be different from what she described in her utterance, suggesting that participants interpreted these utterances non-literally. To further examine the relationship between literal meaning, interpreted meaning, and judgements of irony, we computed the Kullback-Leibler divergence between the literal interpretation of the utterance and the distribution over weather states given the context and utterance. 
%\todo{what is the literal distribution? if it's delta on the word corresponding state, then isn't KL just the interpretation log-prob of the state?}
%\todo[inline]{RDH: I don't quite understand what the KL divergence is measuring in the next paragraph}
%
%We found that adding the KL divergence measure to the linear regression model captured significantly more variance in the irony ratings, with an R-squared of 0.93 (more STATS). 
%\todo{this doesn't seem like the right analysis.... and a 1\% gain is pretty minimal to fuss about.} 
\begin{figure*}[t]
\centering
%\begin{subfigure}{0.7\textwidth}
\scalebox{0.55}{\includegraphics{Plots/irony-human-model-scatter-cropped.pdf}}
\caption{Scatter plot showing correlations between model predictions and human ratings for weather state, speaker valence, and speaker affect. Each dot in a panel represents the interpretation of an utterance in a weather situation, along the dimensions of weather state, valence, and arousal. The darkness of the dots indicate participants' irony ratings for the utterances.}
\label{irony-scatter}
%\end{subfigure}
%\begin{subfigure}{0.43\textwidth}
%\includegraphics[width=220pt, height=140pt]{model-arousal.pdf}
%\caption{Average probabilities of speaker feeling high arousal given her utterance in a weather context.}
%\label{arousal}
%\end{subfigure}
%\begin{subfigure}{0.15\textwidth}
%\includegraphics[width=75pt, height=140pt]{scatters.pdf}
%\caption{Scatter plot of human versus model interpretations.}
%\label{scatter}
%\end{subfigure}
\end{figure*}

\subsubsection{Irony model evaluation}
From Experiment 1a, we obtained the prior probability of a weather state given a context ($P(s)$) as well as the probability of affect given a weather state ($P(A | s)$). In addition, we fit three free parameters to maximize correlation with data from Experiment 1b: the speaker optimality parameter ($\lambda = 1)$ and the prior probability of each of the three QUDs ($P(q_{state}) = 0.3$, $P(q_{valence}) = 0.3$, $P(q_{arousal}) = 0.4$)\footnote{Since $P(q_{state}) + P(q_{valence}) + P(q_{arousal}) = 1$, $P(q_{arousal})$ is determined by the other two QUD parameters and not a free parameter.}.
%\todo{NDG: the three QUD probs aren't independent, so only 3 free params, right?}  
For each of the $45$ utterance and weather context pairs, the model produced an interpretation consisting of the joint posterior distribution $P(s, A | u)$, where $A$ can be further broken down into valence and arousal dimensions. We will examine the model's performance on each of these state and affect dimensions by marginalizing over the other dimensions.

%\todo[inline]{NDG: for each of these correlations also report percent of explainable variance captured with CI (footnote can explain what that is) JTK: still need to do this.}
Figure ~\ref{irony-scatter} shows scatter plots correlating model predictions with human interpretation data for each of the dimensions: weather state, valence, and arousal. %Figure ~\ref{model-state} shows participants' and the model's inferences about the actual weather state given an utterance and a weather context. 
The model predictions of weather state given utterance match humans' interpretations, with a correlation of $0.86$. Since the split-half correlation for the human data is $\rho=0.898$ ($95\%$CI $= [0.892, 0.903]$)\footnote{Split-half correlations $\rho$ were calculated by repeatedly bootstrapping samples from the data (sample each participant with replacement), computing correlation between two halves of the bootstrapped samples, and using the Spearman-Brown prediction formula to estimate predicted reliability with full sample size. Confidence intervals are $95\%$ CI over $1000$ iterations of bootstrap sampling.\label{splithalf}}  we find that our model captures much of the explainable variance in human judgements. The model predicts humans' interpretations of valence extremely well, with a correlation of $0.96$, capturing essentially all of the explainable variance in the data ($\rho = 0.948\pm0.001$).
%Figure ~\ref{valence} shows participants' and the models' inference about the speaker's valence given an utterance and a weather context. From the tight correspondence between model and human interpretations of valence ($r=0.96$), we see that the model is able to incorporate the valence associated with the utterance's literal meaning (e.g. ``The weather is terrible'') and the valence associated with the weather context (e.g. weather context 1) to interpret the probability of the speaker feeling positive valence. 
Importantly, the model infers the appropriate valence even for utterances that are judged as highly ironic (the darker dots in Figure~\ref{irony-scatter}). Thus, the model is able to to recover the intended valence even when it is inconsistent with the valence of the utterance's literal meaning.
%\todo{note that this is the case even when interpreted valence mismatches the prior expected valence given literal meaning -- i.e. the model captures irony in valence.}
The model's predictions for emotional arousal match humans' with a correlation of $0.66$, capturing a substantial amount of the explainable variance ($\rho = 0.763\pm0.005$). %Furthermore, the absolute difference between the model's inferred valence and the valence of the utterance's literal meaning correlates significantly with people's irony ratings ($r = 0.86$, $\rho=0.94 \pm 0.005$), suggesting that the model is able to use inconsistencies between literal and interpreted meanings to identify ironic uses.
Finally, the model's inferences about the QUD varies systematically across the utterances and weather contexts (Figure~\ref{irony-goal-model}. For utterances with high irony ratings (e.g. ``terrible'' uttered in Weather Context 1 and ``amazing'' uttered in Weather Context 9), the model infers that the QUD is most likely the speaker's emotional arousal, whereas for utterances with low irony ratings (e.g. ``terrible'' uttered in Weather Context 9 and ``amazing'' uttered in Weather Context 1), the model infers that the QUD is most likely the  weather state. In fact, the model's posterior probability of an arousal QUD captures participants' graded judgments of irony and is highly correlated with irony ratings ($r = 0.88$, $\rho=0.94 \pm 0.005$). This suggests that the model may be able to use inferences about the QUD to identify ironic uses, and also that the degree of perceived irony may be associated with the probability of affective QUDs such as emotional arousal.
%\todo[inline]{Do we need a figure showing correlations for irony ratings? Not as good as straight-up lm with weather context and utterance as predictors}
%\todo[inline]{JTK: still need to do stats for this}
%Figure~\ref{arousal} shows participants' and the model's inferences about the speaker's arousal. 
%\todo{why is this correlation low? is it because of the terrible-bad inversion i asked about above?}

\begin{table}
\centering
\begin{tabular}{ |c | c | c | c | c | }
  \hline
  \textbf{Model} & \textbf{State} & \textbf{Valence} & \textbf{Arousal} & \textbf{Average} \\\hline                        
  Literal & 0.38 & 0.45 & 0.49 & 0.44\\
  Prior & 0.79 & 0.84 & 0.49 &  0.71 \\
  Valence & 0.84 & 0.79 & 0.61 & 0.75 \\
  Valence + arousal & 0.86 & 0.96 & 0.66 & 0.83\\
  \hline 
  Best possible & 0.90 & 0.95 & 0.76 & 0.87\\\hline
\end{tabular}
\caption{Correlation coefficients between model predictions and human interpretations of weather state, valence, and arousal given an utterance and weather context from Experiment 2. \emph{Best possible} gives an estimate of the maximum possible correlation given noise in the data (see footnote \ref{splithalf}).}
\label{table1}
\end{table}

We considered a series of simpler models to show that the full model using a two-dimensional affect space best predicts human interpretations. 
%\todo{JTK:Should I add a line for split-half in table?}
We first examined a model that interprets utterances literally, such that ``The weather is terrible'' is always interpreted as the weather state being \textit{terrible}, along with the valence and arousal associated with \textit{terrible} weather. 
%Such a model produces a distribution over states that correlates with humans' interpretations with $r=0.38$, inference about valence that correlates with humans' with $r=0.45$, and inferences about arousal with a correlation of $r=0.49$. 
We then examined a model that simply ignores the speaker's utterance and takes into account only the state and affect priors associated with each weather context. 
%Such a model produces a distribution over states that correlates with humans' interpretations with $r=0.79$, inferences abut valence that correlate with $r=0.84$, and inferences about arousal that correlate with $r= 0.49$. 
Finally, we examined the performance of the qRSA model with a unidimensional affect space (valence only). 
Table~\ref{table1} shows the models' correlations with human judgements for state, valence, and affect.
A complete model that takes into account prior knowledge, the literal meaning of the utterance, and a two-dimensional affect space outperforms the other models. This dominance is especially apparent with respect to inferences about valence, which is the most important aspect of understanding an ironic utterance, since the listener must infer the intended positive/negative valence from an ostensibly negative/positive utterance. These comparisons suggest that our full model successfully leverages richer knowledge of affect and uses pragmatic reasoning to produce the appropriate figurative interpretations.
%, both ironic and hyperbolic. 
%Fitting two free parameters to maximize fit with human data, the model obtains a state correlation of $r=0.84$, a valence correlation of $r=0.79$, and an arousal correlation of $r=0.61$.


%\todo[inline]{it would be nice to have some more direct analysis of irony -- that when state/valence mismatches that expected from literal meaning the model can predict it. this may also be a good place to come back to the explicit irony judgements: something like probability the interpretation is on the opposite side of the prior mean? or something more general that would also apply to ironic propositions? oh.. we should totally say something in the discussion about ironic propositions and other non-scalar irony.}

\subsubsection{Discussion}
%\todo[inline]{Add something about common ground and in-group?}
%\todo[inline]{summary of take-home point}
In this section, we formalized intuitions about verbal irony understanding and clarified the role of shared prior knowledge in ironic interpretations. We explored the consequences of expanding the space of affect considered by the qRSA model to account for verbal irony. By making a minimal extension to \citeA{kao2014nonliteral}'s hyperbole model, we were able to capture people's fine-grained interpretations of ironic utterances in addition to hyperbole. This provides evidence that hyperbole and irony may operate using similar underlying principles of communication, namely reasoning about shared background knowledge as well as the speaker's affective attitude.
%, which are not immediately predictable from the literal semantics of an utterance. 
%By providing fine-grained and quantitative manipulations of shared prior knowledge, we may begin to examine why using irony highlights common ground and group membership \cite{gibbs2000irony}.

%The novelty of the paper is to present a way of quantifying the prior expectations and clarifying their role in rational inferences to derive an interpretation that is not at all predictable given the semantically encoded information.

%, which aligns with other informal accounts of the pragmatics of figurative language understanding (cite).  

While our model reproduces the main characteristics of verbal irony, there remain important qualities to account for. 
Researchers have observed that ironic statements that are literally positive but express negative opinions (e.g. saying ``You're a great friend'' ironically to a friend who betrayed you) occur more frequently and are easier to comprehend than statements that are literally negative but express positive opinions (e.g. saying ``You're a terrible friend'' ironically to a friend who helped you) \cite{hancock2002production, jorgensen1984test, clark1984pretense}. This phenomenon, termed the \emph{asymmetry of affect}, is fairly robust and has been addressed by both the pretense theory \cite{clark1984pretense} and echoic mention \cite{jorgensen1984test} theory, as well as through analyses of politeness \cite{dews1995muting}
and humor effects \cite{matthews2006roles}, although a definitive explanation has not been reached. In our experiments, however, we did not observe an asymmetry of affect in the irony ratings or in the interpretation.\todo{stats} This could be because the norms and expectations for weather are not strong enough, or that the weather domain is innocuous enough such that politeness concerns are not an issue. Because we did not observe an affect asymmetry in our data, we did not explore specific ways in which the qRSA model could produce the asymmetry documented in previous studies. In future work, we plan to examine and formalize the ways in which prior expectations or the importance of politeness in different domains may account for varying degrees of affect asymmetry in irony interpretation.
%\todo{We don't see this in our data or model. Different domain? Additional assumptions?}
%\todo{Social motivations for using irony?}

Thus far, the types of encyclopedic knowledge we explored have been restricted to affects associated with various states of the world. We also assumed that the set of QUDs under consideration includes the speaker's attitude or affect towards a state. While this assumption is supported by previous research on the rhetorical effects of hyperbole and verbal irony, it may not generalize to other types of figurative language such as metaphor. In what follows, we will explore ways to systematically elicit the set of QUDs that listeners consider, as well as manipulate prior probabilities over QUDs using discourse context. We show that considering these additional, non-affective QUDs allows the model to capture interesting effects of metaphor interpretation. 

%\todo{Something about why ``terrible'' has lower arousal.}
%For example, speakers often use verbal irony to remind the listener of previous utterances that turned out to be false, or of positive norms that were violated \cite{sperber1981irony, jorgensen1984test}. On the other hand, pretense theory argues that when a speaker produces an ironic utterance, she is only pretending to be someone who would make such an utterance \cite{clark1984pretense}. 
%In this view, recognizing the pretense is central to interpreting verbal irony.
%The latter point in particular has interesting implications on the experiments we present here. While we asked participants to rate the speaker's affect given her utterance, it may be the case that the speaker is only \emph{pretending} to  
%\todo{JTK: Say something about how terrible may involve pretense?} 
%, it does not account for the intuitions behind echoic mention or pretense theories. 
%We hope to enrich our model's understanding of the social aspects of irony by addressing these intuitions in future research. 


%In addition, we aim to further examine how people identify the particular dimensions of meaning that may be under discussion in a given context. For example, affective dimensions such as valence and arousal may be particularly relevant in domains that involve evaluation (e.g. ``good'' or ``terrible'' weather), while non-affective dimensions may be more salient in other domains \cite{kao2014formalizing}.
%\todo[inline]{JTK: say something about non-scalar irony?}

%\todo{JTK: Is this paragraph necessary? Should we instead talk more about future research regarding common ground manipulations and other kinds of affect?}
%Beyond shedding light on the communicative principles underlying irony understanding, our work also has interesting connections to natural language processing. Many researchers aim to automatically detect sarcasm in order to recover the correct sentiment from large bodies of text. %(e.g. ``I was overjoyed to pay $\$30$ for an overcooked steak'') 
%\cite{davidov2010semi, filatova2012irony}. %
%By integrating background knowledge and linguistic meaning in a principled manner, our model provides a deeper understanding of context and common ground that most NLP approaches currently lack \cite{gonzalez2011identifying, wallacehumans}.


%Overall, our experimental paradigm and modeling framework provide a detailed and precise account of irony understanding. Given the prevalence of irony in everyday language and the social functions it serves, we believe it would be \emph{amazing} to understand how people interpret utterances that convey the opposite of what they ostensibly mean ($\#$notsarcastic). 


%\todo[inline]{first review discourse goals; then list and characteristics of metaphor} 
%\todo[inline]{hyperbolic/strikingness (compare with literal utterance); aptness; efficiency}
%\todo[inline]{one study/model for studying each characteristics}
%\todo[inline]{Basic: how the model gets basic metaphorical/figurative interpretation of scalar metaphors}
%\todo[inline]{Compared to literal alternatives, metaphors are more striking/extreme}
%\todo[inline]{Compared to other metaphorical alternatives, some metaphors are better/more apt}
%\todo[inline]{Metaphors communicate more efficiently. Look at feature metaphors}

%Metaphors are utterances that implicitly compare ideas or concepts from different domains \cite{gibbs1999figurative, roberts1994people}. 
%For example, ``Juliet is the sun'' expresses Juliet's beauty; ``My lawyer is a shark'' communicates the lawyer's ruthlessness; and ``Art washes away from the soul the dust of everyday life'' allows Picasso to compare ``art'' to a cleansing fluid and ``the soul'' to a physical object that collects dust, which gracefully accomplishes two poetic metaphors at once. One can find traces of metaphoricity even in mundane utterances such as ``I waited for a long time,'' where the spatial term ``long'' is used to describe the abstract domain of time \cite{lakoff1993contemporary}. Due in part to its ubiquity and in part to the possibility that metaphor is intimately tied to our ability to create mappings between concrete experiences and abstract concepts \cite{lakoff2008metaphors}, metaphor is by far the most widely studied trope in cognitive science and related fields \cite{gibbs2012interpreting}.
%It has inspired a particularly impressive amount of research in cognitive science, spanning topics such as how metaphors structure and shape our thoughts \cite{ortony1993metaphor, lakoff1993contemporary, thibodeau2011metaphors}, whether metaphor processing recruits the same strategies as standard language processing \cite{giora1997understanding, gibbs2002new, glucksberg1993metaphors} and what factors determine people's interpretation a novel metaphor \cite{gentner1997alignment, blasko1993effects, tourangeau1981aptness, kintsch2002metaphor}. This overwhelming interest in metaphor research is due to both the ubiquity of metaphor in everyday language and the potential role of metaphor for helping us understand how the mind creates meaning.
%
%Why do people choose to use metaphors to communicate? What are some characteristics of metaphor that contribute to its popularity? \citeA{roberts1994people} examined the discourse goals that people have when they use various figurative tropes. They found that the most common goals for using metaphor were to clarify ($82\%$), to add interest ($71\%$), to compare similarities ($35\%$), to provoke thought ($35\%$), and to be eloquent ($35\%$). Interestingly, although metaphors are defined as implicit comparisons, ``to compare similarities'' is less frequently listed as a goal than ``to clarify'' and ``to add interest.'' This suggests that beyond examining the cognitive processes for comparing and aligning concepts that may be involved in metaphor understanding, it is also important to consider other higher-level communicative functions that metaphors may serve.
%
%\subsubsection{Subjective attitude}
%Although emotion goals were not listed among the popular discourse goals for metaphor in \cite{roberts1994people}, other researchers have suggested that metaphors are often used to express subjective attitudes towards the subject \cite{ortony1979beyond}. \citeA{riloff2005exploiting} noted that subjective sentences frequently contain figures of speech such as metaphor and hyperbole. Since metaphor describes an ``experienced'' reality rather than actual reality \cite{ortony1975metaphors}, it is perhaps inherently subjective.  By using metaphorical language, speakers may be signaling to the listener that the information conveyed is a product of the speaker's subjective evaluation and not the objective truth.

%\subsubsection{Efficiency}
%In addition to \cite{roberts1994people}'s exploration of discourse goals, other researchers have suggested that metaphorical utterances can be used to efficiently express complex meanings  \cite{ortony1975metaphors, boerger2005variations,glucksberg1989metaphors}. In communication tasks where pairs of participants are separated by a screen and asked to refer to abstract geometrical objects, participants often prefer to describe objects analogically in terms of other known objects rather than use literal analytical descriptions \cite{clark1986referring, glucksberg1989metaphors}. \citeA{fussell1989effects} found that these analogical and figurative descriptions tend to be shorter than literal descriptions. In addition, they found that figurative descriptions are used significantly more often when the intended audience is one's self, where presumably there is a great deal of common ground, than when the intended audience is a different person. These findings suggest that people may be balancing efficiency and clarity when choosing figurative versus literal descriptions. \cite{glucksberg1990understanding} wrote, ``Metaphors are  used  to  communicate a complex, patterned  set of properties in a shorthand that is understood  by the  members of a speech community  who share relevant mutual  knowledge'' (pp. 16). As a result, a potential benefit of speaking in metaphor may be to utilize common ground to communicate both clearly and efficiently. 
%%
%
%In my dissertation, I plan to examine the following (hypothesized) characteristics of metaphorical utterances: (1) metaphors lead to more striking, or extreme, interpretations than literal descriptions (2) metaphors can communicate information along various dimensions with a minimal number of words (3) the interpretation of a metaphor is highly sensitive to the local context (4) the aptness of a metaphor is related to whether there are alternative utterances that can communicate the same amount of information more efficiently.
%To examine these hypotheses, I will test the qRSA model on different types of metaphorical interpretations.
%
%\subsection{Model sketch}
%In our formalization, a listener assumes that the speaker chooses an utterance to maximize informativeness about a subject along dimensions that are relevant to the QUD and consistent with the speaker's communicative goal. 
%\todo[inline]{Todo: Better segue into metaphor that highlights the differences}
%%\todo[inline]{Contributions of metaphor model: (1) Elicit features/QUDs (2) Manipulate QUD through context and show it changes interpretation (3) Show that metaphors communicate more information along different dimensions than literal statements (4) Show that alternative metaphors affect interpretation}
%\todo[inline]{Todo: Run interpretation experiment with free-response features to show that features relevant for interpretation are pretty consistent with features of the source domain (at least for the metaphors we're looking at)}

\subsection{Metaphor}
What kinds of encyclopedic knowledge and QUDs are involved in metaphor interpretation, and how might we formalize them in a qRSA model?
Suppose Ann hears Bob say: ``Cam is a shark''\footnote{While metaphoricity can arise from sentences with various types of syntactic forms, to reasonably limit the scope of our work, here we focus on nominal metaphors of the classic form ``$X$ is a $Y$.''}.
Intuitively, Ann interprets the utterance to mean that Cam is a person who shares certain features with a shark, such as fierceness. To arrive at this interpretation, Ann needs encyclopedic knowledge about features that sharks are likely to have (e.g. fierceness, fins), as well as about people (e.g. fierceness, arms and legs). She also needs to know that the QUD is not which species Cam belongs to, but his characteristics.
%\todo{incomplete}
%It appears that what Bob is communicating 
%How should a listener interpret this utterance?
By incorporating encyclopedic knowledge about the source and target domains and formalizing the idea that speakers may want to communicate specific features of the target, we show that the same qRSA model now produces appropriate interpretations of metaphors.

%Following the qRSA framework, a listener assumes that the speaker chooses an utterance that communicates information  relevant to the QUD. By  
%%Unlike hyperbole and irony, however, these dimensions are not affective in nature. Rather, they are features associated with the metaphorical source, in this case ``shark.''
%In particular, we will focus on three aspects of metaphor understanding that the qRSA model naturally captures. First, interpretation of the same metaphor differs systematically given different discourse contexts, which can be modeled as different prior probabilities over QUDs. Second, metaphors are able to communicate information efficiently along several dimensions and address multiple QUDs at once, which may serve as an advantage over literal statements. 
%Finally, metaphors often communicate more extreme states of the world. \todo{fix this final point}

%specific interpretations of a metaphor are sensitive to prior beliefs, 
%Finally, the specific interpretations of a metaphor are sensitive to the alternative utterances that a speaker could have chosen to address the QUDs under consideration (Experiment 3). 
%show that the RSA model extended to reason about affect and the question under discussion produces non-literal, figurative interpretations that closely match humans'. However, there we assumed that the sets of QUDs under consideration included the speaker's affect, an assumption supported by previous research on the rhetoric effect of hyperbole and verbal irony. In the next section, we will 
%In particular, the model incorporated literal semantics, background knowledge, and assumptions about informativeness in a 


\subsubsection{Model}
To illustrate this idea more concretely and demonstrate how it is implemented in our model, we will continue using the metaphor ``Cam is a shark" as a working example. 
%For simplicity, in this model we restrict the number of possible categories to which a member may belong to $c_a$ and $c_p$, denoting an animal category or a person category, respectively. We also restrict the possible features of John under consideration to a vector of size three: $\vec f = [f_1, f_2, f_3]$, where $f_i$ is either $0$ or $1$ (for example, the three features could be scary, sleek, and finned).\footnote{In principle the model can be extended to accommodate more categories and features.}
We again introduce a literal listener $L_0$, who interprets the utterance as meaning that Cam is literally a member of the category ``shark'' and has corresponding features. 
%Since $L_0$ believes Cam is a shark, she also believes that Cam is likely to have features associated with sharks, for example, being scary or fierce.
The following equation represents the literal listener's interpretation, where $c$ is Cam's category (either a ``person'' or a ``shark''), and $\vec f$ is a vector representation of Cam's features. $P(\vec f | c)$ is thus the prior probability that a member of category $c$ has feature vector $\vec f$.
%The literal listener $L_0$ will interpret the utterance ``Cam is a shark" as meaning that Cam is literally a member of the category ``shark" and has corresponding features. Formally, if $u$ is the uttered category:
%
\begin{equation}
L_0(c, \vec {f} | u) = \left\{ 
  \begin{array}{l l}
    P(\vec {f} | c) & \quad \text{if $c$ = $\llbracket u\rrbracket$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
%
%and conduct behavioral experiments to evaluate the model's performance. We show that peoples' interpretations of metaphors are driven by conversational context and that our model captures this effect. Finally, we show that our model predictions correlate significantly with people's fine-grained interpretations of metaphorical utterances.
%
%
We assume that the speaker's goal is to communicate the values of Cam's features. A QUD is thus a projection from the full feature space to the subset of interest to the speaker. Formally, we represent each QUD as a function $Q_i$, such that $Q_i(\vec f) = f_i$. 
%The QUD may be Cam's species category, or Cam's feature(s). We define the speaker's utility as the negative surprisal of the true state under the listener's distribution, projected along the QUD dimension. 
This leads to the following utility function for speaker $S_1$:
\begin{equation}
U(u | Q, c, \vec f) = \log \sum_{\vec f'} \delta_{Q(\vec f)=Q( \vec f')} L_0(c', \vec f' |u)
\end{equation}
Given this utility function, the speaker chooses an utterance according to a softmax decision rule:
\begin{equation}
S_1(u | Q, c, \vec f) \propto e^{\lambda U(u | Q, c, \vec f)},
\end{equation}
%
Imagine that $S_1$ has the goal to convey $f_1$, scariness, about Cam.
Based on $S_1$'s understanding of $L_0$'s prior knowledge, she knows that if she produces the utterance ``Cam is a shark," $L_0$ will believe that Cam is literally a shark and hence very likely to be \emph{scary}. Since $S_1$'s goal is satisfied if the listener believes that Cam is scary, $S_1$ is motivated to produce such a metaphorical utterance. A pragmatic listener, however, should be able to leverage this pattern to infer that Cam is scary without inferring that Cam is actually a shark.

The pragmatic listener $L_1$ uses Bayesian inference to guess the intended meaning given prior knowledge and his internal model of the speaker:
%To determine the speaker's intended meaning, $L_1$ marginalizes over the possible speaker goals under consideration:
\begin{equation}
L_1 (c, \vec f | u) \propto P(c) P(\vec f | c) \sum_{Q_i}{P (Q_i) S_1 (u|Q_i, c, \vec f)}
\end{equation}
%
If $L_1$ believes it is \emph{a priori} very unlikely that Cam is actually a shark and that $S_1$ may want to communicate about Cam's scariness, she will end up with a posterior distribution where Cam is very likely to be a person who is scary. By combining prior knowledge with reasoning about the QUD, the pragmatics listener can thus arrive at a figurative interpretation of ``Cam is a shark''---Cam is a scary person.

To explore the model's behavior, we make a number of simplifying assumptions. First, we restricted the number of possible categories to which a member may belong to $c_a$ and $c_p$, denoting an animal category (in this case \emph{shark}) or a person category, respectively. We also restricted the possible features of Cam under consideration to a vector of size three: $\vec f = [f_1, f_2, f_3]$, where $f_i$ is either $0$ or $1$. Finally, we assumed a small and rather impoverished set of alternative utterances that the speaker could have said: the utterance she did say (e.g. ``Cam is a shark''), and a grammatically similar and literally true utterance (e.g. ``Cam is a person.'')%(for example, the three features could be scary, sleek, and finned)
.\footnote{In principle, the model can be extended to accommodate more categories, features, and alternative utterances.}
% In Experiment 3, in particular, we explore the model's behavior given more animal categories and alternatives.}

%Imagine that $S_1$ had the goal to convey $f_1$, scariness, about John.
%Based on $S_1$'s understanding of $L_0$'s prior knowledge, she knows that if she produces the utterance ``John is a shark,'' $L_0$ will believe that John is literally a shark and hence very likely to be \emph{scary}. Since $S_1$'s goal is satisfied if the listener believes that John is scary, $S_1$ is motivated to produce such a metaphorical utterance. A pragmatic listener, however, should be able to leverage this pattern to infer that John is scary without inferring that John is actually a shark.
%
%%A pragmatic listener $L_n$ now reasons about such a speaker. Based on prior knowledge, $L_n$ knows that John is extremely unlikely to be literally a member of the shark category. On the other hand, $L_n$ knows that the speaker $S_n$ is fairly likely to want to communicate about John's scariness. $L_n$ also knows that $S_n$ knows that \emph{scary} is a high-probability feature of sharks. 
%
%The listener $L_1$ performs Bayesian inference to guess the intended meaning given prior knowledge and his internal model of the speaker. To determine the speaker's intended meaning, $L_n$ will marginalize over the possible speaker goals under consideration:
%$$
%L_1 (c, \vec f | u) \propto P(c) P(\vec f | c) \sum_{g}{P (g) S_1 (u|g, \vec f)}
%$$
%While speaker and listener could continue to reason about each other recursively, resulting in $L_n$, we restrict ourselves to $L_1$ for present purposes. Past work has shown that this first level of pragmatic reasoning is often a good model of human comprehension.
%%\footnote{The current model does not robustly predict metaphorical interpretations at recursion depths greater than 1. Future work will investigate which features of the model lead to this prediction, and whether this remains true under alternative model definitions.}
%If listener $L_1$ thinks it is likely that speaker $S_1$'s goal is to convey scariness but believes it is \emph{a priori} very unlikely that John is actually a shark, she will determine that $S_1$ is using shark metaphorically---that John is a scary person.

% Table 1
\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\begin{table*}[t]
\centering
\tabcolsep=0.15cm
%\fontsize{11}{9}\selectfont
\begin{tabular}{|a |c c c | c c c|}\hline
%\toprule
\textbf{Animal} & $f_1=1$ & $f_2=1$ & $f_3=1$ & $f_1=0$ & $f_2=0$ & $f_3=0$ \\\hline
ANT & small & strong & busy & large & weak & idle \\
BAT & scary & blind & nocturnal & unalarming & sighted & diurnal \\
BEAR & scary & big & fierce & unalarming & small & nonviolent  \\
BEE & busy & small & angry & idle & large & unangry \\
BIRD & free & graceful & small & unfree & awkward & large \\
BUFFALO & big & strong & wild & small & weak & tame \\
CAT & independent & lazy & soft & dependent & fast & hard \\
COW & fat & dumb & lazy & thin & smart & fast \\
DOG & loyal & friendly & happy & disloyal & unfriendly & unhappy \\
DOLPHIN & smart & friendly & playful & stupid & unfriendly & unplayful\\
DUCK & loud & cute & quacking & quiet & unattractive & non-quacking \\
ELEPHANT & huge & smart & heavy & small & stupid & light \\
FISH & scaly & wet & smelly & smooth & dry & fragrant \\
FOX & sly & smart & pretty & artless & stupid & ugly \\
FROG & slimy & noisy & jumpy & nonslippery & quiet & relaxed \\
GOAT & funny & hungry & loud & humorless & full & quiet \\
%%%%
GOOSE & loud & mean & annoying & quiet & nice & agreeable \\
HORSE & fast & strong & beautiful & slow & weak & ugly \\
KANGAROO & jumpy & bouncy & cute & relaxed & inelastic & unattractive \\
LION & ferocious & scary & strong & nonviolent & unalarming & weak \\
MONKEY & funny & smart & playful & humorless & stupid & unplayful \\
OWL & wise & quiet & nocturnal & foolish & loud & diurnal \\
OX & strong & big & slow & weak & small & fast\\
PENGUIN & cold & cute & funny & hot & unattractive & humorless \\
PIG & dirty & fat & smelly & clean & thin & fragrant \\
RABBIT & fast & furry & cute & slow & hairless & unattractive \\
SHARK & scary & dangerous & mean & unalarming & safe & nice \\
SHEEP & wooly & fluffy & dumb & hairless & hard & smart\\
TIGER & striped & fierce & scary & unpatterned & nonviolent & unalarming  \\
WHALE & large & graceful & majestic & small & awkward & inferior \\
WOLF & scary & mean & angry & unalarming & nice & unangry \\
ZEBRA & striped & exotic & fast & unpatterned & native & slow\\
\hline

\end{tabular}
\caption{$32$ animal categories, feature adjectives, and their antonyms. Feature adjectives were elicited from Experiment 1a and indicate when a feature is present ($f_i = 1$). Antonyms were generated using WordNet and indicate when a feature is not present ($f_i = 0$). Feature sets shown in Experiment 2b were created with this table, where $\vec f = [1, 0, 0]$ for category ``ant'' is represented by the words $\{$small, weak, idle$\}$. There are $2^3 = 8$ possible feature combinations for each animal category.}
\end{table*}
% End of Table 1

Based on this formulation, the listener needs to consider the following prior probabilities to arrive at an interpretation: 
\begin{itemize}
\item[(1)] $P(c)$: the prior probability that the entity discussed belongings to category $c$. We assume that the listener is extremely confident that the person under discussion (e.g. Cam) is a person, but that there is a non-zero probability that Cam is actually a non-human animal. We fit $P(c_a)$ to data with the assumption that $10^{-4} \leq P(c_a) \leq 10^{-1}$.
\item[(2)] $P(\vec f | c)$: the prior probability that a member of category $c$ has feature values $\vec f$. This is empirically estimated in Experiment 2a.
\item[(3)] $P(Q)$: the probability of a question under discussion $Q$. This prior can change based on the conversational context that a question sets up. For example, if the speaker is responding to a vague question about Cam, e.g. ``What is Cam like?'', the prior over goals is uniform. If the question targets a specific features, such as ``Is Cam scary?'', then the speaker is much more likely to have the goal of communicating John's scariness. However, she may still want to communicate other features about Cam that were not asked about. We assume that when the question is specific, the prior probability that $S_1$'s goal is to answer the specific question is greater than $0.5$, fitting the value to data below.
\end{itemize}

To evaluate our model's interpretation of metaphorical utterances, we selected a set of $32$ metaphors comparing human males to various non-human animals. We conducted Experiment 2a and 2b to elicit feature probabilities for the categories of interest. We then conducted Experiment 2c to measure people's interpretations of the set of metaphors. 
\subsubsection{Experiment 2a: Feature Elicitation}
We selected $32$ common non-human animal categories from an online resource for learning English (\url{www.englishclub.com}). The full list is shown in Table $1$.
$100$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant read $32$ animal category names presented in random order, e.g. ``whale'', ``ant'', ``sheep''. For each animal category, participants were asked to type the first adjective that came to mind in a text box. 
Using participants' responses, we constructed a list of adjectives for each animal category and ordered them by the number of times they were given by a different participant (i.e. their popularity). We removed all color adjectives, such as ``white'' and ``black,'' to eliminate the possibility of interpreting these adjectives as racial descriptions. To avoid redundancy in the feature set, we used WordNet \cite{miller1995wordnet} to identify synonymous adjectives and only kept the most popular adjective among a set of synonyms. We then took the three most popular adjectives for each animal category and used them as the set of features. In what follows, $f_1$ is the most popular adjective, $f_2$ the second, and $f_3$ the third. Table 1 shows the animal categories and their respective features.

\subsubsection{Experiment 2b: Feature Prior Elicitation}
Using the features collected from Experiment 1a, we elicit the prior probability of a feature vector given an animal or person category (i.e. $P(\vec f | c)$). We assume that the adjective corresponding to a feature (e.g. \emph{scary}) indicates that the value of that feature is $1$ (present), while the adjective's antonym indicates that the value of that feature is $0$ (not present). We used WordNet to construct antonyms for each of the adjective features produced in Experiment 1a. When multiple antonyms existed or when no antonym could be found on WordNet, the first author used her judgment to choose the appropriate antonym. Table 1 shows the resulting list of antonyms. For each animal category, eight possible feature combinations were constructed from the three features and their antonyms. For example, the possible feature combinations for a member of the category ``ant'' are \{small, strong, busy\}, \{small, strong, idle\}, \{small, weak, busy\}, and so on.

$60$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed $16$ trials in random order. Each trial consisted of the eight feature combinations for a particular animal category. Using slider bars with ends marked by ``Impossible'' and ``Absolutely certain,'' participants were asked to rate how likely it is for a member of the animal category to have each of the eight feature combinations. Participants also rated the probabilities of the feature combinations for a male person. We only elicited priors for males to minimize gender variation and to maintain consistency with Experiment 2c.

We normalized each participant's ratings for the eight feature combinations in a trial to sum up to 1 based on the assumption that the feature combinations exhaustively describe a member of a particular category. Using the Spearman-Brown prediction equation, reliability of the ratings was  $0.941$ ($95\%$ CI $=[0.9408, 0.9414] $). Averaging across participants' normalized ratings, we obtained feature priors $P(\vec f | c)$ for $c = c_a$ (animal) and $c = c_p$ (person).
%, where $f_i = 1$ is represented by the feature adjective and $f_i = 0$ is represented by its antonym. 
%Figure~\ref{prior} shows the average marginal probabilities of features being present given an animal category and a person category. 
Since the features were created using the animal categories in Experiment 1a, by construction features are rated as significantly more likely to be present in the animal category than in the person category ($F(1, 190) = 207.1$, $p < 0.0001$). These results confirm that participants are fairly confident that each animal category has certain distinguishing features (mean$= 0.61$, sd$=0.06$), while those same features are rated as appearing in people less often (mean$=0.48$, sd$=0.06$).

%\begin{figure}[t]
%\begin{center}
%\scalebox{0.6}{\includegraphics{Plots/priors_bar.pdf}}
%\end{center}
%\caption{Average marginal prior probability ratings for the three features given an animal category or a person category. Error bars are standard error over the $32$ items.} 
%\label{prior}
%\end{figure}

\subsubsection{Experiment 2c: Metaphor Interpretation}
We created $32$ scenarios based on the animal categories and results from Experiment 2a. In each scenario, a person (e.g. Bob) is having a conversation with his friend about a person that he recently met. Since we are interested in how contextually determined QUDs affect metaphor interpretation as well as the meanings of metaphorical versus literal utterances, we created four conditions for each scenario by crossing vague/specific QUDs and literal/metaphorical utterances. In vague QUD conditions, Bob's friend asks a vague question about the person Bob recently met: ``What is he like?'' In specific QUD conditions, Bob's friend targets $f_1$ and asks a specific question about the person: ``Is he $f_1$?,'' where $f_1$ is the most popular adjective for a given animal category $c_a$. In literal conditions, Bob replies with a literal utterance, either by saying ``He is $f_1$.'' to the question ``What is he like?'' or ``Yes.'' to the question ``Is he $f_1$?''. In Metaphorical conditions, Bob replies with a metaphorical statement, e.g. ``He is a $c_a$.'' where $c_a$ is an animal category. See Table 2 for examples of each condition.

\begin{figure*}[t]
\begin{center}
\scalebox{0.7}{\includegraphics{Plots/metaphor-interp-prior.pdf}}
\end{center}
\caption{The leftmost panel shows participants' average feature probability ratings given a literal utterance (Experiment 2c), subtracted by the average prior probabilities of a person having each of the features \emph{a priori} (Experiment 2b); the middle panel shows participants' average feature probability ratings given a metaphorical utterance, subtracted by the feature prior; and the rightmost panel shows the model's posterior probabilities for each of the features given a metaphorical utterance, again subtracted by the feature prior. Error bars are $95\%$ confidence intervals. %The model facet shows model posterior probabilities for each of the features given a metaphorical utterance and a vague/specific QUD, also subtracted by the average prior probabilities of a person having each of the features. 
Compared to literal utterances, participants' interpretation of metaphorical utterances shows two qualitative effects: (1) feature probabilities are all significantly higher than the prior, meaning metaphors communicate information abut multiple features at once, while literal utterances only communicate information about Feature 1 (2) interpretation is sensitive to the QUD, such that specific QUDs boost the amount of information communicated about a specific feaature. The model captures both qualitative effects.} 
\label{metaphor-human-summary}
\end{figure*}

\begin{table}[h]
\tabcolsep=0.2cm
\small
\begin{tabular}{|llll|}\hline
QUD & Utterance & Example question & Example utterance \\\hline
General & Literal & ``What is he like?'' & ``He is scary.'' \\
Specific  & Literal & ``Is he scary?'' & ``Yes.'' \\
General & Metaphorical & ``What is he like?'' & ``He is a shark.'' \\
Specific & Metaphorical & ``Is he scary?'' & ``He is a shark.'' \\\hline
\end{tabular}
\caption{Example questions and utterances for each of the four experimental conditions in Experiment 2c.}
\end{table}

$49$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed $32$ trials in random order. The $32$ trials were randomly and evenly assigned to one of the four conditions, i.e. each participant read $8$ scenarios for each condition. For each trial, participants used sliders to indicate the probabilities that the person described has features $f_1$, $f_2$, and $f_3$, respectively.

%\begin{figure}[ht]
%\begin{center}f
%\scalebox{0.6}{\includegraphics{Plots/model_literal.pdf}}
%\end{center}
%\caption{Model's average marginal category probabilities given a metaphorical utterance ($P(c | u)$). Model's prediction for the animal category is close to $0$ and close to $1$ for the person category. Error bars are standard error over the $32$ items.} 
%\label{scatter_full}
%\end{figure}

For each condition of each scenario, we obtained the average probability ratings for the three features. The left-most and middle panels in Figure~\ref{metaphor-human-summary} show the average ratings for each feature across animal categories given a literal or metaphorical utterance and a vague or specific QUD, subtracted by the prior feature probabilities obtained in Experiment 2b. When the speaker gives a literal statement directly affirming the presence of $f_1$, participants rate $f_1$ as significantly more likely than when the speaker gives a metaphorical statement ($F(1, 126) = 52.6$, $p < 0.00001$). However, participants rate $f_2$ and $f_3$ as significantly more likely when the speaker produces a metaphorical utterance than when the utterance is literal ($F(1, 126) = 23.7$, $p < 0.0001$; $F(1, 126) =13.66$, $p < 0.0005$). Comparing feature probability ratings in Experiment 2c to the feature priors obtained in Experiment 2b, we can measure how literal and metaphorical utterances change listeners' inferences about a person's features. Given a literal utterance that directly confirms the existence of $f_1$, probability ratings for $f_1$ are significantly higher than the prior probabilities of $f_1$ for a person ($t(63) = 59.19, p < 0.00001$). However, probability ratings for $f_2$ and $f_3$ are not significantly different from their prior probabilities ($t(63) = -0.13, p = 0.89$; $t(63) = 0.03, p = 0.97$). Given a metaphorical utterance, probability ratings for all three features are significantly higher than the prior probabilities ($t(63) = 15.74, p < 0.0001$; $t(63) = 7.29, p < 0.0001$; $t(63) = 5.91, p < 0.0001$). This analysis suggests that metaphorical utterances may convey richer information and update listeners' beliefs along more dimensions than literal utterances.

We now analyze the effect of contextually determined QUDs on the interpretation of literal or metaphorical utterances. When the speaker's utterance is literal, the probability ratings for $f_1$, $f_2$, and $f_3$ are not significantly different given a vague or a specific question ($(F(1, 62)= 2.73, p =0.1; F(1, 62)=0.0001, p = 0.99; F(1, 62) < 0.0001, p = 0.99$). For metaphorical utterances, however, the question type has an effect on participants' interpretations: participants rate the probability of $f_1$ as significantly higher when the question is specifically about $f_1$ than when it is vague ($F(1, 62) = 10.16$, $p < 0.005$). The probabilities of $f_2$ and $f_3$ are not significantly different given a vague question or a specific question about $f_1$ ($F(1, 62) = 0.04$, $p > 0.05$; $F(1, 62) = 0.8285$, $p > 0.05$). This suggests that people's interpretation of metaphor may be more sensitive to the QUDs set up by context than their interpretation of literal utterances. The left-most and middle panels in Figure~\ref{metaphor-human-summary} show these effects.

\subsubsection{Metaphor model evaluation}
We used the feature priors obtained in Experiment 2b to compute model interpretations of the $32$ metaphors. As discussed in the previous section, the behavioral results in Experiment 2c show evidence that the context set up by a question changes participants' interpretation of a metaphor. Our model naturally accounts for this using the speaker's prior over QUDs $P(Q)$. When a speaker is responding to a vague question, we set the prior distribution for $P(Q)$ as uniform. When the speaker is responding to a question specifically about $f_1$, we assume that $P(Q_1) > 0.5$ and equal between $P(Q_2) = P(Q_3)$. Fitting the goal prior parameter to data yields a prior of $P(Q_1) = 0.9$ when responding to a specific question about $f_1$. We fit the category prior $P(c_a) = 0.01$ and the speaker optimality parameter $\lambda = 4$. %note that saying ''g_1'' is a slight abuse of notation given the model definitions above. 

Using these parameters, we obtained interpretation probabilities for each of the $32$ metaphors under both vague and specific QUD conditions. For each metaphor and QUD condition, the model produces a joint posterior distribution $P(c, \vec f | u)$. We first show a basic but important qualitative result, which is that the model is able to interpret utterances metaphorically. Marginalized over values of $\vec f$, the probability of the person category given the utterance is close to one ($P(c_p | u) = 0.994$), indicating that the pragmatic listener successfully infers that the person described as an animal is actually a person and not an animal. This shows that the model is able to combine prior knowledge and reason about the QUD to arrive at metaphorical interpretations.

%\begin{figure}[t]
%\begin{center}
%\scalebox{0.6}{\includegraphics{Plots/model_bar.pdf}}
%\end{center}
%\caption{Model's average } 
%\label{scatter_full}
%\end{figure}

\begin{figure*}[t]
\begin{center}
\scalebox{0.4}{\includegraphics{Plots/metaphor-human-model-scatter-withWords.pdf}}
\end{center}
\caption{Model predictions ($x$ axis) vs participants' probability ratings ($y$ axis) for $192$ items ($32$ metaphors $\times$ $3$ features $\times$ $2$ goal conditions). Color indicates goal condition and shape of points indicates feature number.} 
\label{scatter_full}
\end{figure*}

We now turn to the second component of the interpretation, $P(\vec f | u)$. 
%Figure X shows the average marginal feature probabilities given a vague or specific goal. The model's interpretations are shaped by the communicative goal set up by context, where $f_1$ receives significantly higher marginal probabilities when the speaker is responding to a specific question about $f_1$ ($F(1, 62) =17.92$, $p<0.0001$). This qualitatively matches behavioral results in Figure X. 
To quantitatively evaluate the model's performance, we correlated model predictions with human interpretations of the metaphorical utterances. Given a metaphorical utterance and a vague or specific QUD condition, we computed the model's marginal posterior probabilities for $f_1$, $f_2$, and $f_3$. We then correlate these posterior probabilities with participants' probability ratings from Experiment 2c. Figure~\ref{scatter_full} plots model interpretations for all metaphors, features, and QUD conditions against human judgments. Correlation across the $192$ items ($32$ metaphors $\times$ $3$ features $\times$ $2$ QUD conditions) is $0.64$ ($p < 0.001$)\footnote{We also fit a free parameter ($\alpha=3$) that determines a power-law transformation of the feature priors, where the probability of each feature set was raised to the power of $alpha$ and renormalized to sum up to $1$. The power-law transformation was introduced based on analyses of the feature priors showing that participants tend to overestimate unlikely features and underestimate likely features, likely due to a tendency to avoid the extreme ends of the slider bars. Without performing the power-law transformation and using the raw normalized feature priors obtained in Experiment 2a, the model correlation is $0.6$ ($p < 0.001$)}. The predicted reliability of participants' ratings using the Spearman-Brown prediction formula is $0.828$ ($95\%$ CI $=[0.827, 0.829]$), suggesting first that people do not agree perfectly on metaphorical interpretations, and second that our model captures a significant amount of the reliable variance in the behavioral data. In particular, our model does especially well on predicting participants' judgments of $f_1$, which are the most salient features of the animal categories and were targeted by specific questions in Experiment 2c. Correlation between model predictions and human judgments for $f_1$ is $0.77$ ($p < 0.0001$), while the predicted reliability of participants' ratings for $f_1$ is $0.82$ ($95\%$ CI $=[0.818, 0.823]$). 

We now compare our model's performance to a baseline model that also considers the feature priors and the conversational context. We constructed a linear regression model of participants' feature ratings that takes as predictors the marginal feature priors for the animal category, the marginal feature priors for the person category, whether the QUD is vague or specific, and their interactions. With eight parameters, this model produced a fit of $r= 0.48$, which is significantly worse than our model ($p < 0.0001$ on a Cox test). This suggests that the qRSA model adequately combines people's encyclopedic knowledge as well as principles of pragmatics to produce metaphorical interpretations that match behavioral data.


\subsubsection{Discussion}
In this section, we showed that metaphorical interpretations can be captured by the general communicative principles formalized in the qRSA model. In particular, our model captures several important aspects of metaphor interpretation. First, the model goes beyond the literal meanings of utterances to infer non-literal interpretations (e.g., Cam is a person and not a shark). Second, the model provides quantitative judgments about the person's features based on fine-grained encyclopedic knowledge of the metaphorical source and target (e.g. sharks are very likely scary; people are very unlikely to be striped). Finally, the model produces different interpretations given different local conversational contexts (e.g. is the topic of conversation specifically about Cam's scariness, or his characteristics more generally?) in a manner that matches human responses. 
%Third, the model successfully captures the empirical finding that metaphors tend to communicate information along more dimensions than literal statements. 
%Finally, a model that takes into account the alternative utterances (both literal and metaphorical) that a speaker could have produced to address specific QUDs predicts people's interpretations with higher quantitative accuracy than one that does not.
 %Furthermore, behavioral results show that the interpretation of a metaphor is shaped in part by the conversational context, which our model naturally accounts for using communicative goals. 
 
Although our model predictions provide a reasonable fit to behavioral data and outperforms a linear regression model using fewer parameters, we observed residual variance that can be further addressed. Fig.~\ref{scatter_full} shows the $10$ metaphor-feature pairs that have the highest residual variance in the regression model. Examining these data points in more detail, we can identify the following potential reasons for these errors.

One potential source of error is the fact that most of the features in this dataset are gradable adjectives (e.g. \emph{strong}, \emph{large}, \emph{soft}) whose meanings vary with context and the types of entities they predicate. For example, a strong ant is probably less strong than a strong person, and even less strong than a strong ox. In addition, our judgment of whether an ant is \emph{strong} depends on the class of entities that we compare it to. An ant can be strong compared to other living things of its size, but not strong compared to all living things in general. In our experiments and model, we did not consider the context-sensitivity of these gradable features. In Experiment 2b, participants rated an ant as being strong with probability $0.69$ on average, but rated a person as being strong with probability $0.54$, presumably because participants are judging \emph{strong} with respect to a class of insects in the \emph{ant} case, and with respect to a class of human beings in the \emph{person} case. In the qRSA model, a pragmatic listener using these priors will interpret ``Cam is an ant'' to mean that Cam is more likely than an average person to be strong ($P(\text{strong}) = 0.7$), because an ant in more likely than a person to be strong. However, the intuitive interpretation of the metaphor is that Cam is less strong than an average person, presumably because people have the encyclopedic knowledge that when the comparison class includes human beings, an ant is very unlikely to be considered \emph{strong}. By using context-sensitive gradable adjectives as features without providing the model with correct context-sensitive feature probabilities, we prevent the from producing the appropriate interpretation for metaphors such as ``Cam is an ant.'' 

%Two reasons may contribute to these types of errors. The first is that the priors are not calibrated on an absolute scale (strong for an animal of an ant's size, not strong absolutely). 
%Another reason why the model has difficulty predicting certain data points accurately has to do with the feature probabilities we elicit in Experiment 2b. In Experiment 2b, we asked participants to rate how likely it is that a member of an animal category (e.g. ``ant'') has various combinations of features (e.g. $\{$ small, strong, busy $\}$). Importantly, lack of calibration . For a human being, 

Another related source of error is the set of alternative utterances that the model considers. 
Previous work has shown that alternative utterances---what the speaker could have said---can strongly affect listeners' interpretation of what the speaker \emph{did} say \cite{bergen2012s}. In this experiment, we did not take into account the range of alternative utterances (both literal and metaphorical) that a listener considers when interpreting a speaker's utterance. 
In the model simulations presented here, we only considered two syntactically similar utterances that a speaker could choose to say: ``Cam is a [\emph{animal}, e.g. whale]'', and ``Cam is a person.'' Because the prior probability of a whale being \emph{graceful} ($P(\text{graceful} | \text{whale}) = 0.7$) is higher than the prior probability of a person being \emph{graceful} ($P(\text{graceful} | \text{person}) = 0.44$), a speaker is more likely to choose ``Cam is a whale'' over ``Cam is a person'' to communicate that Cam is \emph{graceful}. As a result, a pragmatic listener modeled by qRSA would interpret ``Cam is a whale'' to mean that Cam is more graceful than the average person ($P(\text{graceful}) = 0.6$). However, if the speaker wanted to communicate that Cam is \emph{graceful}, intuitively it seems that a different metaphor would be more effective at addressing the QUD, for example ``Cam is a swan.'' Since \emph{graceful} is a higher probability feature for ``swan'' than for ``whale,'' the listener reasons that if the speaker had intended to communicate the feature \emph{graceful}, he would have said ``He is a swan'' since it optimally satisfies that goal. Since the speaker did \emph{not} produce the utterance ``He is a swan,'' the listener infers that \emph{graceful} is a less probable feature of Cam. 
We posit that allowing the model to reason over a more realistic set of alternative utterances would produce interpretations that more closely match humans'.

%The model assigns high probabilities for features that are strongly associated with the metaphor source (e.g. \emph{ants} being \emph{strong}, and \emph{whales} being \emph{graceful}), but it appears that humans do not infer high probabilities of these features from the animal metaphors (e.g. ``John is an ant'' is not likely interpreted as \emph{John is strong}, and ``John is a whale'' is not likely interpreted as \emph{John is graceful}). (strong, ox; slow, turtle).
%
%Previous work has shown that alternative utterances---what the speaker could have said---can strongly affect listeners' interpretation of what the speaker \emph{did} say \cite{bergen2012s}. In this experiment, we did not take into account the range of alternative utterances (both literal and metaphorical) that a listener considers when interpreting a speaker's utterance. We posit that other plausible alternative utterances may account for some of the variance in the data that our model does not capture. Consider the metaphor ``He is an ant'' and the corresponding features \emph{small, strong}, and \emph{busy}. Our model currently assigns a high probability to the feature \emph{strong} given the metaphor, while participants assign it a lower probability. Indeed, this data point has the highest residual in our model fit. To test the idea that alternative utterances may account for this discrepancy, we construct a model that has ``He is an ox'' as an alternative utterance. ``Ox'' has features that roughly align with the features of ``ant'': \emph{strong, big}, and \emph{slow}. Since \emph{strong} is a higher probability feature for ``ox'' than for ``ant,'' the listener reasons that if the speaker had intended to communicate the feature \emph{strong}, she would have said ``He is an ox'' since it optimally satisfies that goal. Since the speaker did \emph{not} produce the utterance ``He is an ox,'' the listener infers that \emph{strong} is a less probable feature. Adding this alternative utterance to the model indeed lowers the marginal posterior probability of \emph{strong} given the utterance ``He is an ant.'' Based on this simple error analysis, we posit that adding alternative utterances across all animal categories could help improve our model's performance. 

%Conventionality
Beyond the pragmatic effects of alternative utterances and gradable adjectives, another source of error we observe in our data is the issue of conventional or idiomatic metaphors, whose meanings are no longer directly derived from features of the metaphorical source. For example, the metaphorical meaning of ``fox'' is associated with phrases such as ``silver fox'' or ``foxy lady,'' which describe people who are attractive in a way that could be described as \emph{pretty}. Even though the prior probabilities of a man or a fox being \emph{pretty} are not very high ($0.49$ and $0.67$, respectively), the conventionalized metaphorical meaning of ``fox'' carries a stronger connotation of attractiveness, and may result in a higher probability rating for \emph{pretty} given the metaphor.

%Nonliteral features
Finally, we observe that some features may themselves have nonliteral interpretations. For example, it is quite unlikely for a person to literally ``quack'' like a duck quacks ($P(\text{quack} | \text{person}) = 0.21$). However, a person could reasonably make noises or have a voice that resembles quacking in its loudness. While humans have the flexibility of interpreting the features metaphorically and assign a higher probability to Cam being \emph{quacking} given the utterance ``Cam is a duck'' ($0.54$), the model is influenced by the low prior probability of a person literally having the feature (as well as the inference that the QUD is likely about Cam's other features) and ends up with a lower posterior feature probability ($0.1$). In addition, in the case of ``Cam is a cat,'' we observe that the feature \emph{soft} when applied to a cat means something quite different when it is applied to a person. When applied to a cat it refers to the physical softness of a cat's fur or body, while when applied to a person it refers to the person's flexibility or gentleness in personality. Since cats are not strongly associated with flexible, gentle personalities, the metaphor ``Cam is a cat'' does not seem to adequately address the QUD of Cam's \emph{softness} (of personality), and may thus feel less appropriate to human participants. These types of metaphors raise an interesting question about how people identify the appropriate features that are beings transferred from the source to the target domain, especially when some features may have very different meanings when applied in the two domains. 
%Taking into account the This raises the interesting question of how nonliteral interpretations of features are derived to begin with. Emergent features?

Despite important open questions regarding the context-sensitivity of features, the role of alternative utterances, and conventional metaphors, our model takes a promising first step towards formalizing principles of pragmatics in order to predict metaphor interpretation. In particular, we show that several key characteristics of metaphor---dependence on prior knowledge, ability to communicate information along several dimensions, and sensitivity to local context---can be explained by modeling speakers as rational and cooperative agents who wish to be informative and relevant with respect to the QUD.
%
%showing that basic principles of communication can shape metaphor interpretation in important ways. 
%which can be formalized in a general computational framework that assumes speakers to be rational and cooperative agents.
In the next section, we will begin to address some of these remaining issues (specifically, the issue of feature calibration and nonliteral features) using the case study of hyperbolic metaphors and relational metaphors. \todo{make sure we're actually doing that}


%%\todo[inline]{Transition to introduce Experiment 3}
\subsection{Alternative Utterances}

In order to adequately interpret an utterance, listeners often need to reason about a set of alternative utterances that the speaker could have said but did not. 
For example, in order to interpret the utterance ``Some of the apples are red'' as the pragmatically enriched \emph{some but not all} of the apples are red, listeners need to consider the fact that the speaker could have said the more informative utterance ``all of the apples of red'' if it were indeed true. 
%Since the speaker did \emph{not} say ``All of the apples were red,'' it is likely that ``all'' is not.
While the importance of alternative utterances is supported by a great deal of psycholinguistic evidence \cite{bergen2012s, krifka2007approximate, degen2015availability}, how listeners arrive at a reasonable set of alternative utterances is still an open question \cite{fox2011characterization, ciardelli2015alternatives}. For our purposes, we take the approach of constructing alternative utterances based on the set of QUDs under consideration. Since we assume that the QUDs considered for metaphor interpretation are related to the features associated with the metaphorical source, we assume that the alternative utterances a listener considers in order to interpret a metaphor are in turn associated with those features. More concretely, suppose a speaker produced the metaphor, ``Cam is an ant.'' Intuitively, the interpretation of this metaphor (how likely it is that Cam is small, strong, busy, etc.) will be influenced by other animal metaphors the speaker could have produced to communicate information about Cam's size, strength, and industriousness. 

To test whether adding a richer set of alternative utterances to the qRSA model can improve its performance, we conducted Experiment 3a and 3b to elicit alternative animal metaphors as well as associated feature priors. 
%We conducted Experiment 3a and 3b to elicit alternative animal metaphors and explore its effect on the 

\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\begin{table*}
\centering
\tabcolsep=0.15cm
%\fontsize{11}{9}\selectfont
\begin{tabular}{|a |c c c | c c c|}\hline
%\toprule
\textbf{Animal} & $f_1$ & $f_2$ & $f_3$ & $a_1$ & $a_2$ & $a_3$ \\\hline

ANT   & small & strong & busy  & mouse & ox & bee\\
BUFFALO & big & strong & wild & elephant & ox & tiger\\
CAT    & independent & lazy & soft  & wolf & sloth & rabbit\\
DUCK   & loud & cute & quacking   &   lion & rabbit&dog\\
FOX  &   sly & smart & pretty  &      snake & dog &cat\\
OX  &  strong & big & slow & horse& elephant &turtle\\
SHEEP &  wooly & fluffy & dumb     &  mammoth & dog &cow\\
WHALE &  large&graceful&majestic &elephant & swan&horse\\\hline
\end{tabular}
\label{features-alternatives}
\caption{Eight original animals, their features features, and alternative animals that are strongly associated with those features.}
\end{table*}


\subsubsection{Experiment 3a: Alternative Elicitation}

Given the large space of animal categories and features selected in Experiment 2, constructing a complete set of alternative utterances using the full set of 32 metaphors was not feasible and would result in a very large and unwieldy set of animals and features. Instead, we focused on a smaller set of initial animal categories in order to elicit the set of alternative metaphors a listener may reasonably expect a speaker to produce. We focused on the eight animal metaphors that had the highest residual variance in the linear regression model shown in Figure~\ref{scatter_full}. In what follows, we will refer to these eight animals as the \emph{original animals}. %In the next section, we describe Experiment 3, where we begin with a set of $12$ animal metaphors and elicit alternative metaphors to examine their effect on the model's behavior.

%We selected 12 common animals that have various distinguishing features: \textit{ant, whale, bird, elephant, panda, monkey, penguin, giraffe, cheetah, turtle, lion}, and \textit{rabbit}, which we will call the core animals. 
Each of the eight original animals have three features elicited in Experiment 2a, as shown in Table~\ref{features-alternatives}. For each of the features, we elicit animals that are strongly associated with those features. 
$36$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant saw $21$ unique feature adjectives presented in random order. Following the same paradigm as Experiment 2a, participants were asked to type in a text box the first animal that came to mind that could be described by that adjective. For each original animal and each feature adjective, we identified the most popular animal associated with the feature. We ensured that each original animal has exactly three unique alternative animals by removing duplicates. Table~\ref{features-alternatives} shows the set of alternative animals elicited in this manner. 
%We removed color adjectives and combined synonymous adjectives by consulting WordNet \cite{miller1995wordnet}. We then identified the two most popular adjectives for each animal category and used those as features \footnote{In Experiment 3, we decided to use two features for each category instead of three due in part to the difficulty of obtaining reliable ratings from human participants for $2^3=8$ possible feature combinations, and in part to restrict the set of alternative utterances we elicit in Experiment 3b to a reasonable size.}.
%
%\subsubsection{Experiment 3b: Alternative Elicitation} 




%Experiment 3a yielded $15$ unique features elicited from the $12$ core animals: $2$ features for each core animal, with $9$ overlapping features across the $12$ animals such as ``small,'' ``big,'' and ``strong.'' $50$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk to produce animals associated with each of the $15$ features. Each participant read the $15$ feature adjectives presented in random order, with the prompt: ``Write down the first animal you can think of that is \underline{\hspace{1cm}},'' where \underline{\hspace{1cm}} is a feature adjective. We tallied the number of times an animal was produced for each of the features and identified the animals most strongly associated with each feature.
%
%For each of the $12$ core animals we started with, we now have $2$ associated features, as well as a set of animals associated with those features. For each core animal, we constructed a set of alternative animals by selecting $2$ animals most strongly associated with each feature, excluding the core animal itself. If there were animals that were strongly associated with both of the two features, we selected an additional animal associated with the first feature, in order to ensure that each core animal had exactly $4$ alternative associated animals. The full set of core animals, features, and alternative animals are shown in Table~\ref{alternatives}. 
%
\subsubsection{Experiment 3b: Feature Prior Elicitation}
Given the alternative animals collected from Experiment 3a, we now elicit the prior probability of a feature vector given an animal category (i.e $P(\vec{f} | c)$). For each original animal, we elicited the prior probabilities of the eight feature combinations described in Experiment 2b for the original animal, its three alternatives, and a human person.
%For each set of three features (e.g. $\{$small,strong,busy$\}$), we again constructed antonyms to indicate when the feature is absent, resulting in four possible feature combinations (e.g. $\{$small, industrious$\}$, $\{$small, lazy$\}$,  $\{$big, industrious$\}$, and $\{$big, lazy$\}$). For each feature set, we elicit prior feature probabilities for the following categories: the core animal (e.g. \emph{ant});  the alternative animals (e.g. \emph{mouse, dog, beaver}, and \emph{monkey}); and a human male.

$35$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed $10$ trials in random order. Each trial consisted of the eight feature combinations for a particular animal category. Using slider bars with end points marked by ``very unlikely'' and 	``very likely,'' participants were asked to rate how likely it is for a member of the category to have each of the eight feature combinations. Based on the assumption that the feature combinations exhaustively describe a member of a particular category, we normalized each participant's ratings for the four feature combinations within a trial to sum up to 1.

\subsubsection{Model simulation}
We used the same model and QUD assumptions described in the Metaphor section. For this model simulation, we expanded the set of utterances $U$ that a speaker considers given a particular QUD. For an original metaphorical utterance $u$ such as ``Cam is an ant'' and a particular QUD $Q_i$, we define the set of utterances $U$ considered by the speaker to be $U=\{u, a_i, a_0 \}$, where $a_0 = \text{``Cam is a person''}$, $a_1 = \text{"Cam is a mouse''}$, $a_2 = \text{``Cam is an ox''}$, and $a_3 = \text{``Cam is a bee''}$. 

\begin{figure*}
\begin{center}
\scalebox{0.6}{\includegraphics{Plots/metaphor-alternatives.pdf}}
\end{center}
\caption{Probabilities of a feature given an animal metaphor, as predicted by the original model, a model that considers alternative utterances, and participant responses in Experiment 2c.} 
\label{metaphor-alternatives}
\end{figure*}
 
%
%\subsubsection{Experiment 3d: Metaphor Interpretation}
%$45$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk.
%\todo[inline]{Describe results.}
%\subsubsection{Model evaluation}
%\todo[inline]{Model comparison with and without alternative animals.}
%
%In Study 2, we use a series of experiments to show that some metaphorical utterances such as ``Bob is an ox'' allow speakers to communicate about multiple dimensions of Bob at once---for example, that Bob is strong, big, and slow. In order to communicate the same amount of information using a literal description, a speaker would need to choose a much longer and costlier utterance, such as: ``Bob is strong, big, and slow.'' We show that listeners infer information along more dimensions given a metaphorical utterance than given a single literal description. We then show that the qRSA model is able to capture these differences between literal and metaphorical utterances.

%In Study 3, we use a series of experiments to show that the interpretation of metaphorical utterances such as ``Bob is an ox'' is highly sensitive to the local conversational context. For example, if Liz asks: ``What is Bob like?'' and Sam replies: ``Bob is an ox,'' Liz will believe that Bob is perhaps strong, or big, or slow. However, if Liz instead asks: ``Is Bob strong?'' and Sam replies: ``Bob is an ox,'' Liz will be much more likely to believe that Bob is strong. On the other hand, regardless of Liz's initial question, the interpretation of literal utterances such as ``Bob is strong'' is unlikely to vary with the conversational context. We show that this effect can be modeled in qRSA using different priors over QUDs.


\subsection{Hyperbolic Interpretations of Metaphor}


Metaphors often elicit hyperbolic interpretations and result in more extreme beliefs about the world.
In this section, we examine people's interpretations of metaphors such as ``Cam is a giraffe,'' where the salient feature being communicated (e.g. height) lies on a continuous scale. We find that the interpretation of these types of metaphors demonstrates two effects: First, given the utterance ``Cam is a giraffe,'' listeners believe that Cam is taller than they do when given the literal description ``Cam is tall.'' Second, listeners believe that Cam is taller than the average person, but much shorter than the average giraffe. We show that the qRSA model naturally captures these effects.
\subsubsection{Model}
\subsubsection{Experiment 4a: Prior Elicitation}
\subsubsection{Experiment 4b: Metaphor Interpretation}
\subsubsection{Model Evaluation}
\subsubsection{Discussion}

%\todo[inline]{Basic: how the model gets basic metaphorical/figurative interpretation of scalar metaphors}
%\todo[inline]{Compared to literal alternatives, metaphors are more striking/extreme}

%\subsubsection{Experiments}

%\subsubsection{Model}

%\subsection{Study 2: Non-paraphrasability}


%\subsubsection{Experiments}

%\subsubsection{Model}

%\subsection{Study 3: Context-sensitivity}
%\todo[inline]{Metaphors communicate more efficiently. Look at feature metaphors}



%\subsubsection{Experiments}

%\subsubsection{Model}

%\subsection{Study 4: Aptness and alternatives}
%
%%\todo[inline]{Compared to other metaphorical alternatives, some metaphors are better/more apt}
%
%Many researchers have examined the factors that contribute to the ``aptness'' of a metaphor as well as how aptness facilities metaphor processing \cite{tourangeau1981aptness, jones2006roosters, blasko1993effects}. In Study 4, we propose that aptness may in part be related to two pragmatic factors: (1) The presence of multiple salient features of the metaphorical target, which cannot be described literally without using longer utterance (2) The absence of salient alternative metaphors that the speaker could have used to communicate about the dimension(s) under discussion. We will test these ideas using a series of experiments, and show that the qRSA model can capture some of these effects.

\subsection{Towards a Model of Relational Metaphor}

In the studies described above, we focused on metaphors where the main thing being communicated is the feature. that communicate 

Definitely need to cite Holyoak's pragmatic accounts.

``The multiconstraint theory postulates that goals and context affect every stage of analogical inference'' (Holyoak, 1984, 1985; Spellman Holyoak, 1996)

Three examples that demonstrate QUDs at different levels of abstraction. Instead of communicating about features of an entity, communicating about relationships the entity has with other entities. 

Distribution over graphs. 


\subsubsection{Feature QUDs}

\subsubsection{Relational QUDs}


The immune system is an army. An entity can be seen as a node in a graph, where the edges are relations. Relational concept. 

The immune system is an army
We assume that there are 3 components in the source domain: ["army", "country", "enemy"] and 3 components in the target domain: ["immune system", "body", "virus"]. 

We have prior knowledge about the following relations in the source domain: protects(army, country); harms(enemy, country); fights(army, enemy), and prior knowledge about just one relation in the target domain: harms(virus, body). We can represent each domain as a distribution over labeled directed graphs, where each labeled edge is a relation between two of the components. The QUD is thus which labeled directed graph represents the immune system domain.

Given the analogy "The immune system is an army," a QUD that specifies which labeled directed graph describes the target domain, and the prior knowledge that harms(virus, body), the pragmatic listener infers the correct directed labeled graph and assigns "virus," "body," and "immune system" to the correct nodes of the graph. This results in an alignment between the three components in the target graph and the three components in the source graph.

\subsubsection{Feature and Relational QUDs}

The atom is a solar system
In this particular implementation I assumed that there are 2 planets and 2 electrons, but we can make this more general by only caring about sets of planets and electrons so the number of components doesn't matter.

We again represent each domain as a labeled directed graph, where the nodes are components e.g. "sun", "planet", "nucleus", "electron." Unlike the immune system metaphor, here we only consider one type of relation: orbits(x, y). We don't have any prior knowledge about which components orbit which other components in the target domain.

However, we do have prior knowledge that orbits(planet, sun); that the planets are small and the sun is big; and that the electrons are small and the nucleus is big. The QUD can be  "which labeled directed graph describes the target domain?", "what are the features of each of the nodes in the graph?", or both.

Given the analogy "The atom is a solar system," the pragmatic listener assigns highest posterior probability to the correct directed labeled graph and assigns "nucleus", "electron1", "electron2",.... to the correct nodes of the graph, such that orbits(electron, nucleus). This is thus a correct alignment between the components in the target graph and the components in the source graph.

The two models work in the same way, with some minor differences in implementation. The main difference is that in the immune system case, the alignment relies on prior knowledge about the relation between two components of the target domain (harms(virus, body)). In the atom case, the alignment relies on prior knowledge about features of the components in the target domain (nucleus is relatively big, electrons are relatively small), and the fact that the speaker may want to communicate both the surface features of the components and the relational structure.

It feels like these two examples illustrate how RSA can be extended to handle structured metaphors and alignments in a way that could make sense to the structure mapping people. If this sounds reasonable, I will start writing it up in a section in the figurative paper and make some potential figures. 

\section{General Discussion}
In this paper, we reviewed approaches to studying figurative language understanding from the perspective of pragmatics and communication. We described the communicative principles and extralinguistic factors that shape figurative understanding and articulated the need to clarify the interactions among these components in order to more precisely understand how people arrive at figurative meanings in context.
Building upon the RSA framework, we proposed and evaluated a computational model that explicitly describes how people use different sources of information---literal meaning, encyclopedic knowledge, and contextual information---to produce figurative interpretations. Through a series of behavioral experiments, we showed that the model closely matches people's interpretation of hyperbole, verbal irony, and metaphor, suggesting that different types of figurative language may share the same underlying communicative principles. 

%Based on these results, we argue that the qRSA model incorporates information in a principled and theoretically motivated manner, which provides a useful framework for testing and modeling various phenomena in language understanding. 
%
%In what follows, we will discuss two specific areas in which the qRSA model has made (and will continue to make) contributions.

%\subsection{Computational models of language understanding}
%In recent years, computational models have grown increasingly popular in cognitive psychology and linguistics. The appeal of computational approaches for these fields has roots in both science and engineering. From a scientific perspective, the formal rigor of computational models allows researchers to define theories in a more precise manner, test these theories against empirical data at a higher resolution, and arrive at more accurate and powerful conclusions. From an engineering perspective, implementing formal theories of human behavior in computer programs enables us to build artificial agents that can predict and replicate these behaviors at a large scale. 

We believe that the qRSA model makes several critical advancements to formal models of human language understanding. The model captures key intuitions about communication, including the role of common ground between listener and speaker, the assumption that speakers produce utterances that maximize informativeness, and the idea that informativeness should be considered with respect to the question under discussion. By formalizing these intuitions, the model is able to go beyond the literal meanings of utterances and predict subtleties in interpretation that are sensitive to background knowledge, communicative efficiency, local context, and alternative utterances. From a scientific perspective, our work provides a formal definition of QUD as well as the relevance principle, which allows us to empirically test the prediction that listeners reason about utterances' relevance to the QUD in order to arrive at appropriate interpretations. This also allows us to show that QUD inference is critical for many instances of language understanding, in particular figurative language, where the literal meanings of utterances are often false. By exploiting listeners' assumption that speakers choose utterances in order to communicate information relevant to the QUD (and not irrelevant information or information that is already in common ground), speakers can choose utterances that are not literally true (e.g. ``Cam is a giraffe'') in order to effectively communicate relevant information (e.g. that Cam is unusually tall), without leading the listener to believe false information (e.g. that Cam is a giraffe). 
%
From an engineering perspective, our model provides a step towards building systems that use pragmatic reasoning and representations of the QUD to better interpret and generate utterances in context. We believe that our approach may contribute to the design of artificial agents that use language in more flexible and creative ways.

%a better understanding of how we use pragmatic reasoning to incorporate considering these components can result in dialogue systems that can exploit researchers advancement  should be integexplicitly accounts for QUD and contributes to contributions to NLP, dialogue systems, better model of QUD and extralinguistic knowledge \todo{make this whole paragraph more reasonable}

%\subsection{Explanations for figurative language}
In addition to advancing general models of language understanding, our work may provide explanations for phenomena that particularly interest researchers of figurative language.
One distinctive aspect of figurative utterances is that they communicate multiple dimensions of meaning at once and are often difficult to paraphrase \todo{cite}. 
By considering encyclopedic knowledge such as affects associated with different states and features associated with different categories, our model naturally accounts for the multi-dimensional quality of figurative meaning. 
%
In addition, many researchers have observed that the interpretation of figurative language is especially sensitive to common ground \cite{pexman2004does, gibbs2000irony}. Given that the literal meanings of figurative utterances are often implausible or false, listeners rely on background knowledge in order to reason about and recover speakers' intended meanings. As a result, it is important for listeners and speakers to share background knowledge (both encyclopedic knowledge and specific prior beliefs) in order to communicate successfully using figurative utterances. Indeed, researchers have found that willingness to use ironic utterances is positively correlated with social intimacy between interlocutors \cite{kreuz1996use}, and interlocutors who use metaphorical speech with each other are perceived as having a closer relationship\cite{horton2007metaphor}. \citeA{kreuz1996use} attributes the effects of social closeness to what he calls a principle of inferability: speakers are more likely to use a non-literal utterance when they are more certain that it will be understood appropriately, which in turn is more likely when the speakers and listeners share similar background knowledge and beliefs. For example, suppose Ann saw a watch that cost $\$1000$ and wants to tell Bob that she thinks it is very expensive. In order to communicate her meaning effectively using a hyperbolic utterance: ``That watch cost a million dollars!'', Ann must be fairly confident that Bob's prior beliefs would not lead him to believe that the watch literally cost a million dollars. And since Bob reasons about Ann's motivation for choosing an utterance, given such a hyperbolic utterance, Bob arrives at the inference that Ann must be fairly confident about his prior beliefs. Our model provides a natural way to incorporate inferences about common ground in order to predict and model effects of social closeness.  In its current form, our model assumes that the listener is certain that the speaker has perfect knowledge of the listener's background knowledge and prior beliefs. If we relax this assumption to incorporate uncertainty about the speaker's knowledge of the listener, the pragmatic listener is able to derive information about the speaker's knowledge of the listener. Preliminary explorations of our model show that it naturally affords these types of social inferences, which we plan to examine more thoroughly in future work.

%predict when and how listeners infer common ground with the speaker based Because our model explicitly represents the listener's encyclopedic knowledge and prior beliefs, it provides a natural way to extend the model to incorporate infereces
%to The importance of background knowledge
%\subsubsection{Common ground}
%A third 
%Figurative language is especially sensitive to common ground. People tend to use figurative language with people they are close to. 
%Our model formalizes the importance of common ground and prior knowledge. In addition, it makes further predictions abut what happens when listeners are uncertain about common ground.

One of the most important contributions of our work is that it unifies the interpretation of diverse types of figurative language in a single computational model. The generality of this model suggests that separate processing mechanisms may not be necessary to derive different types of figurative interpretations, or to derive figurative interpretations at all. 
%According to our model, listeners consistently incorporate multiple sources of information in order to recover speakers' intended meanings. 
The qRSA model does not distinguish \emph{a priori} between figurative language and other types of language use; instead, the same pragmatic reasoning takes place regardless of whether the model ultimately produces a figurative or a literal interpretation. While our model is a computational-level account of language understanding and makes no process-level claims, we note that the model engages the same reasoning mechanism to interpret both figurative and literal utterances, which is consistent with psycholinguistic evidence that figurative language does not take reliably longer to process. On the other hand, some evidence suggests that prior context reduces the processing time for figurative utterances. Although we again do not make claims with our model regarding processing times, we suggest that this type of contextual effect may be modeled as a reduction of uncertainty in the QUD, which may lead to faster processing. 
%Since the literal meanings of figurative utterances are often implausible or false, listeners need to rely on other sources of information in order to reason about and recover speakers' intended meanings, including the QUD, encyclopedic knowledge, and prior beliefs. Speakers need to 
%For example, the context-sensitivity of figurative interpretation can be modeled as sensitivity to the QUD. Prior context sets up QUD. Since 
%context, figurative meanings are accessed as quickly or even more quickly than literal meanings.
%researchers have observed that the interpretation of figurative utterances is highly sensitive to context. Given enough prior context, figurative meanings are accessed as quickly or even more quickly than literal meanings.
%Figurative utterances such as metaphor receive different interpretations under different contexts. Metaphorical meanings are accessed more quickly when given enough prior context. The qRSA models this using QUD.
%Furthermore, our model provides a formal 
%Researchers who study figurative language have observed several different things about figurative language understanding, which our model captures and may be able to explain.
%
%\subsubsection{Aptness judgements}
Overall, our work suggests that the rich meanings expressed by figurative language can be explained by basic principles of communication, thus demonstrating the importance of considering pragmatics in theories of figurative language.
\todo[inline]{Not sure whether it's dangerous to mention processing times at all, but I think reviewers will likely ask about it, so might as well say something...}
%The fact that the same basic communicative principles can be formalized to produce fine-grained interpretations of diverse types of figurative language demonstrates the importance of considering the role of pragmatics in shaping many aspects of linguistic meaning. 

%\subsection{Social aspects of communication}
%
%\subsection{Why do people use figurative language?}
%\subsubsection{Intimacy and common ground inference}
%\subsubsection{Humor}


%In addition, previous work has shown that conventional metaphors such as ``He is a pig'' may be processed differently from novel metaphors \cite{bowdle2005career}, which introduces a set of interesting questions to investigate with our model. We believe that our computational framework advances understanding of the computational basis of metaphor and of communication more generally, and we hope that it will continue to shed metaphorical light on related questions.

\subsection{Future directions}

%While the RSA framework provides a promising start to examining figurative language understanding, it also inspires
%many more questions. 
%Each of these components inspire 
%The work we described formalizes figurative language understanding and provides a precise and promising start to examining creative, figurative language use in a concrete and rigorous manner. 
The work we described invites many directions for future research, both for computational models of pragmatics and figurative language understanding. %that can be pursued using both our modeling framework and experimental paradigm.
%\subsubsection{Open questions in the RSA framework}
%How are QUDs determined? How are alternative utterances determined?
While our model explicitly reasons over a set of QUDs and alternative utterances, we have not precisely defined how listeners select the particular set of QUDs and alternatives over which to reason. For example, when should listeners consider speakers' affects and subjective attitudes to be potential QUDs? How do listeners decide that certain features of the metaphorical source are likely QUDs (e.g. \emph{scary} in the case of ``Cam is a shark''), while others are not (e.g. \emph{has teeth} or \emph{swims})? In the work described in this paper, we made the simplifying assumption that QUDs arise from the literal meanings of utterances in an associative manner, and relied on intuition and previous research to focus on affective QUDs in the case of hyperbole and irony and feature QUDs in the case of metaphor. We then defined a set of QUDs based on the affects and features associated with various states and categories. Future research should examine whether QUDs can be more systematically inferred from prior context or the utterance itself, thus removing the need to predefine a set of QUDs that the model should consider for a given type of figurative utterance. A more flexible and general way to define a set of QUDs could also enable us to determine which types of QUDs are most effectively addressed by different kinds of figurative utterances. 

Metaphor understanding is related to many other complex issues such as analogy, conventionality, and embodied cognition. To reasonably limit the scope of our work, in this paper we focused on simple nominal metaphors such as ``Cam is a shark,'' where both the metaphor source and target are concrete objects, and where the source is relatively unconventional (for example, we did not include idiomatic metaphors such as ``Cam is a chicken''). We also only considered attributional metaphors, where the source and target have certain features in common (e.g. ``fierce'' and ``scary''). We have not yet shown that our model can account for other types of metaphors, such as (1) verbal metaphors, where verbs instead of nouns are used non-literally (e.g. ``The ice-skater \textit{flew} across the rink'') (2) conventional metaphors, or metaphors that appear frequently in everyday language to the point of becoming ``dead'' or lexicalized (e.g. ``The man was \textit{drowning} in sorrow''; ``She's the \textit{head} of the household''). (3) relational metaphors, where the source and target share the same relational structure but not necessarily the same simple attributes (e.g. ``A child's brain is a \textit{sponge}'' means that a child's brain absorbs information the way sponge absorbs water) (4) abstract attributional metaphors, where the source and target only share attributes in an abstracted sense (e.g. ``Cam is a \textit{rock}'' means that Cam is stable and dependable in his personality, but not necessarily physically static). \todo{(3) and (4) may turn out to be the same thing.}
\todo[inline]{
Future work will need to look into these other types...need to fill in ideas regarding how.
}

%We make rather vague assumptions that $\vec m$ arises from $m_0$ in an associative manner, and that the distribution over QUD is sensitive to the local context. Based on the principle of informativity, we also assume that the QUD is unlikely to be about something that the listener already knows. 
%For example, since Liz knows that Bob is a person, it is unlikely that Sam wants to communicate the species to which Bob belongs. 
%To understand the model's behavior more fully, these details should be examined in future research. 
%In this work, we showed that the qRSA model successfully captures three types of figurative interpretation: hyperbole, irony, and metaphor. 
While our model provides a unified explanation for three diverse uses language: hyperbole, irony, and metaphor, future work should examine whether the model extends to other types of figurative language. 
For example, understatement refers to the use of a mild statement to describe an extreme situation, such as saying ``It's a bit rainy today'' in the middle of a rainstorm in order to draw attention to the extreme raininess. Because the literal meaning of ``a bit rainy'' is associated with both neutral valence and low arousal, our model finds very little reason for a speaker to choose this utterance to communicate either negative valence or high arousal. What information, then, is a speaker trying to communicate using a statement that doesn't match the speaker's valence, arousal, or the objective state of the world? We observe that intuitively, understatements such as ``It's a bit rainy today'' draw attention to the common ground between speaker and listener, such as the fact that they both know that it is extremely rainy. In order to explain other types of figurative language such as understatement, it may be necessary to incorporate these types of common ground and social inferences. \todo{idioms?}.

Thus far, our work has focused on information-theoretic motivations for using figurative language, such as communicating efficiently about multiple QUDs at once, as well as communicating extreme states. In future work, we plan to further explore the social motivations for using figurative language, such as to communicate common ground with the listener or to evoke humor. Earlier in the discussion, we briefly described how figurative language may lead to inferences about common ground and outlined a way to incorporate these inferences in our model. In addition, we observe that figurative language is often funny, which also has social consequences. We believe the humor of figurative language can be explained by the Incongruity Theory, which posits that situations that afford multiple incongruous at the same time are more likely to be funny. We speculate that figurative utterances are often funny because they give rise to different interpretations given different QUDs, and hypothesize that the humor of figurative utterances can be predicted by formalizations of incongruity. %\todo{politeness?}

%How might we use the qRSA model to explain higher-level social functions of nonliteral language, such as humor and conveying social closeness? 
%Are listeners less likely to interpret an utterance figuratively when there is little common ground between speaker and listener? 


%\subsubsection{Other types of figurative language}
%Can the model explain metaphor interpretation that requires feature alignment and identification of analogical relations? 

%Understatement, Idioms. Understatement may require common ground inference. Idioms require a way to represent conventionalized meanings and compositionality.

%We aim to address these questions in future research to further clarify how communication principles interact to produce metaphorical meaning. 
%Can the model generalize beyond metaphor and hyperbole to explain other types of figurative language such as irony? Can the model provide a natural account of metaphor aptness and poeticness? 

%\subsubsection{Social functions}


\section{Conclusion}
%Figurative language is a crowded and somewhat confusing space. 
Figurative language presents a puzzle for communication. While the information encoded in the literal semantics is false or trivial, figurative utterances are often evocative, socially meaningful, and highly informative. In this paper, we provided an account of figurative language understanding that partially solves this puzzle using a computational model of communication.
%We described a computational model that formalizes figurative communication as recursive reasoning between speaker and listener. 
We showed that a Rational Speech-Acts model extended to accommodate inferences about the QUD successfully recovers true and relevant information from literally false utterances, predicting people's interpretations of hyperbole, irony, and metaphor with high accuracy. We argue that the qRSA model incorporates information in a principled and theoretically motivated manner, providing a useful framework for testing and modeling various phenomena in language understanding. 
By formalizing principles of communication to explain figurative language, our work 
%advances formal models of language understanding and 
sheds light on how our linguistic, cognitive, and social faculties work together to produce figurative meanings that expand the expressiveness of language.
%Based on these results, we also suggest that We also argue that the 
%suggesting that the same   that there is plenty of room for formal models to carve out various elements and produce explicit, computational-level hypotheses about which elements are necessary as well as how they interact. We argue that the RSA theory provides a useful framework that incorporates information in a principled manner and fits naturally within a general theory of language understanding. By introducing elements central to figurative language 
%such as associated meanings, affect, and communicative intent, 
%this paradigm may advance formal models of language understanding and sheds light on how our linguistic, social, and world knowledge work together to produce meaning.
%together enables us to we add figurative flesh to the bare literal bones of language.


%
%In what follows, we will discuss two specific areas in which the qRSA model has made (and will continue to make) contributions.

\bibliographystyle{apacite}
\bibliography{bibliography.bib}


\end{document}

%% 
%% Copyright (C) 2011-2014 by Brian D. Beitzel <brian at beitzel.com>
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%% 
%% http://www.latex-project.org/lppl.txt
%% 
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%% 
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%% 
%% 
%% This work is ''maintained'' (as per LPPL maintenance status) by
%% Brian D. Beitzel.
%% 
%% This work consists of the file  apa6.dtx
%% and the derived files           apa6.ins,
%%                                 apa6.cls,
%%                                 apa6.pdf,
%%                                 README,
%%                                 APAamerican.txt,
%%                                 APAbritish.txt,
%%                                 APAdutch.txt,
%%                                 APAenglish.txt,
%%                                 APAgerman.txt,
%%                                 APAngerman.txt,
%%                                 APAgreek.txt,
%%                                 APAczech.txt,
%%                                 APAendfloat.cfg,
%%                                 apa6.ptex,
%%                                 TeX2WordForapa6.bas,
%%                                 Figure1.pdf,
%%                                 shortsample.tex,
%%                                 longsample.tex, and
%%                                 bibliography.bib.
%% 
%%
%% End of file `./samples/longsample.tex'.
